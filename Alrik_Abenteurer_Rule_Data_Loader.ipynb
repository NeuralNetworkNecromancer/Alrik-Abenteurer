{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdJau9Ri0PSEJ3M2XW4hIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuralNetworkNecromancer/Alrik-Abenteurer/blob/main/Alrik_Abenteurer_Rule_Data_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####INITIATE####\n",
        "!pip install qdrant-client openai google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests PyPDF2 pytz\n",
        "\n",
        "# utils\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from IPython.display import display, clear_output, Markdown, HTML\n",
        "from google.colab import drive, files, auth, userdata, output\n",
        "import ipywidgets as widgets\n",
        "import pytz\n",
        "\n",
        "#data loading packages\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import io\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify folder and file details\n",
        "folder_dir = f'/content/drive/MyDrive/DSA/Alrik Abenteurer'\n",
        "app_dir = f'{folder_dir}/App'\n",
        "targeted_folder = f'{folder_dir}/Rules'\n",
        "company_name = 'alrik'\n",
        "\n",
        "# Setup Openai\n",
        "OPENAI_API_KEY = userdata.get('openai')\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# OAuth2.0 Authentication\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# If modifying these SCOPES, delete the file token.json.\n",
        "creds = None\n",
        "if os.path.exists('token.json'):\n",
        "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = google.auth.default()\n",
        "\n",
        "# Build the Drive and Docs services\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "# Build the Google Sheets API client.\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "3zg74TZcoz4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####LOAD DATA####\n",
        "\n",
        "# Function to recursively fetch files from a folder and its subfolders\n",
        "def fetch_files_from_folder(folder_id, path=\"\"):\n",
        "    files_data = []\n",
        "    # Fetch files in the current folder\n",
        "    results = drive_service.files().list(\n",
        "        q=f\"'{folder_id}' in parents\",\n",
        "        fields=\"files(id, name, mimeType, parents)\"\n",
        "    ).execute()\n",
        "    items = results.get('files', [])\n",
        "\n",
        "    for item in items:\n",
        "        # Check if item is a folder, recursively fetch its contents\n",
        "        if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "            subfolder_id = item['id']\n",
        "            subfolder_path = f\"{path}/{item['name']}\"\n",
        "            files_data.extend(fetch_files_from_folder(subfolder_id, subfolder_path))\n",
        "        else:\n",
        "            # Fetch file details and content for non-folder items\n",
        "            file_data = fetch_file_data(item, path)\n",
        "            if file_data:\n",
        "                files_data.append(file_data)\n",
        "\n",
        "    return files_data\n",
        "\n",
        "# Function to fetch data for a single file\n",
        "def fetch_file_data(file_item, folder_path):\n",
        "    try:\n",
        "        file_id = file_item.get('id')\n",
        "        file_name = file_item.get('name')\n",
        "        file_url = f\"https://docs.google.com/document/d/{file_id}\"\n",
        "\n",
        "        # Log the file name and URL\n",
        "        print(f\"Processing file: {file_name}, URL: {file_url}\")\n",
        "\n",
        "        # Fetch file content if it's a Google Doc\n",
        "        if file_item['mimeType'] == 'application/vnd.google-apps.document':\n",
        "            doc = docs_service.documents().get(documentId=file_id).execute()\n",
        "            content_text = extract_text_from_doc(doc)\n",
        "\n",
        "            return {\n",
        "                \"file_id\": file_id,\n",
        "                \"file_name\": file_name,\n",
        "                \"folder_path\": folder_path,\n",
        "                \"file_url\": file_url,\n",
        "                \"content_text\": content_text\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_item.get('name')}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Function to extract text from a Google Doc\n",
        "def extract_text_from_doc(doc):\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "    return \"\".join(content)\n",
        "\n",
        "# Adjust targeted_folder to be relative to the Google Drive root\n",
        "relative_targeted_folder = targeted_folder.replace('/content/drive/MyDrive/', '')\n",
        "\n",
        "# Function to fetch the ID of the root folder and then the specific target folder\n",
        "def fetch_folder_id(target_folder):\n",
        "    # Split the target folder path\n",
        "    folder_names = target_folder.strip('/').split('/')\n",
        "\n",
        "    # Start from the root of the drive\n",
        "    parent_id = 'root'\n",
        "    folder_id = None\n",
        "\n",
        "    for folder_name in folder_names:\n",
        "        query = f\"name = '{folder_name}' and '{parent_id}' in parents and mimeType = 'application/vnd.google-apps.folder' and trashed = false\"\n",
        "        results = drive_service.files().list(\n",
        "            q=query,\n",
        "            fields=\"files(id, name, mimeType)\"\n",
        "        ).execute()\n",
        "\n",
        "        items = results.get('files', [])\n",
        "        if not items:\n",
        "            print(f\"Folder '{folder_name}' not found under parent ID {parent_id}.\")\n",
        "            return None\n",
        "\n",
        "        folder_id = items[0]['id']  # Assume the first result is the correct one\n",
        "        print(f\"Found folder '{folder_name}' with ID: {folder_id}\")\n",
        "        parent_id = folder_id  # Set the current folder as the parent for the next iteration\n",
        "\n",
        "    return folder_id\n",
        "\n",
        "# Fetch the folder ID of the targeted folder\n",
        "folder_id = fetch_folder_id(relative_targeted_folder)\n",
        "if folder_id:\n",
        "    # Fetch files data from the targeted folder and its subfolders\n",
        "    files_data = fetch_files_from_folder(folder_id)\n",
        "\n",
        "    # Save to a JSON file\n",
        "    with open(f'{app_dir}/{company_name}_drive_data.json', 'w') as f:\n",
        "        json.dump(files_data, f, indent=4)\n",
        "    print(f\"Saved data for {len(files_data)} files to {company_name}_drive_data.json\")\n",
        "else:\n",
        "    print(f\"Target folder '{relative_targeted_folder}' not found.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WL2vVQEYGwzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####CREATE EMBEDDINGS####\n",
        "import openai\n",
        "import hashlib\n",
        "\n",
        "# consistent means there is a risk of collusion when used for multiple customer datasets stored in the same environment. Solution is to add a customer unqiue value pre or post hashing\n",
        "def consistent_hash(s):\n",
        "    \"\"\"Hashes a string and returns a consistent integer.\"\"\"\n",
        "    # Get a SHA256 hash of the string\n",
        "    result = hashlib.sha256(s.encode()).hexdigest()\n",
        "    # Convert the first 8 characters of the hash to an integer\n",
        "    return int(result[:8], 16)  # Converts the hex to an integer\n",
        "\n",
        "# Load the combined data\n",
        "with open(f'{app_dir}/{company_name}_drive_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Set up OpenAI with the API key\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Function to create embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    # Use the instantiated client to create an embedding\n",
        "    response = client.embeddings.create(input=text, model=model)\n",
        "    # Access the embedding attribute directly\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding\n",
        "\n",
        "\n",
        "\n",
        "# Create embeddings and prepare for upload\n",
        "embeddings_data = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"content_text\") or \"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not text:\n",
        "        print(f\"Skipped item '{item['file_name']}' due to empty 'content_text'.\")\n",
        "        continue\n",
        "\n",
        "    # Check if ID is a number or alphanumeric and handle accordingly\n",
        "    try:\n",
        "        # Try converting it directly\n",
        "        item_id = int(float(item[\"file_id\"]))\n",
        "    except ValueError:\n",
        "        # If direct conversion fails, hash it\n",
        "        item_id = consistent_hash(item[\"file_id\"])\n",
        "\n",
        "    # Create a shallow copy of the item to avoid mutating the original data\n",
        "    item_payload = item.copy()\n",
        "\n",
        "    embedding = get_embedding(text)\n",
        "    embeddings_data.append({\n",
        "        \"id\": item_id,\n",
        "        \"vector\": embedding,\n",
        "        \"payload\": item_payload  # Add the payload directly here\n",
        "    })\n",
        "\n",
        "    print(f\"Successfully embedded item '{item['file_name']}' with original ID {item['file_id']} (hashed ID: {item_id}).\")\n",
        "\n",
        "# Save embeddings to JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'w') as file:\n",
        "    json.dump(embeddings_data, file, indent=4)\n",
        "\n",
        "print(f\"\\n\\n------------------------------------------------------------------\\n\\nSaved {len(embeddings_data)} embeddings to embeddings_data.json.\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3n5owskB-ghf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UPSERT TO VECTOR DB\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import json\n",
        "\n",
        "# Setup Qdrant\n",
        "QDRANT_API_KEY = userdata.get('qdrant')\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=userdata.get('TBU_qdrant_url'),\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Load the embeddings data from JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'r') as file:\n",
        "    embeddings_data = json.load(file)\n",
        "\n",
        "# Function to split data into smaller batches\n",
        "def split_into_batches(data, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i + batch_size]\n",
        "\n",
        "# Batch size - you can adjust this based on your requirements and limitations\n",
        "BATCH_SIZE = 30  # Example batch size\n",
        "\n",
        "# Define the collection name you want to use\n",
        "collection_name = f\"{company_name}_embeddings\"\n",
        "\n",
        "try:\n",
        "    # Fetch a list of all collections\n",
        "    all_collections = qdrant_client.get_collections()\n",
        "\n",
        "    # Check if the collection name exists in the list of collections\n",
        "    if collection_name in all_collections:\n",
        "        # If it exists, delete the current collection\n",
        "        qdrant_client.delete_collection(collection_name=collection_name)\n",
        "        print(f\"Deleted existing collection '{collection_name}'.\")\n",
        "    else:\n",
        "        print(f\"Collection '{collection_name}' does not exist. Creating new collection.\")\n",
        "\n",
        "    # Define vectors configuration for the new collection\n",
        "    vectors_config = models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "\n",
        "    # Create the new collection\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=vectors_config\n",
        "    )\n",
        "    print(f\"Created new collection '{collection_name}'.\")\n",
        "\n",
        "    # Split embeddings data into batches and upsert each batch\n",
        "    for i, batch in enumerate(split_into_batches(embeddings_data, BATCH_SIZE)):\n",
        "        try:\n",
        "            qdrant_client.upsert(points=batch, collection_name=collection_name)\n",
        "            print(f\"Uploaded batch {i + 1} ({len(batch)} embeddings) to Qdrant in the '{collection_name}' collection.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while uploading batch {i + 1}: {e}\")\n",
        "            if hasattr(e, 'response'):\n",
        "                print(\"Error response:\", e.response.text)\n",
        "\n",
        "    print(f\"Finished uploading all batches to Qdrant in the '{collection_name}' collection.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    if hasattr(e, 'response'):\n",
        "        print(\"Error response:\", e.response.text)\n"
      ],
      "metadata": {
        "id": "7FyaAxr4BaJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}