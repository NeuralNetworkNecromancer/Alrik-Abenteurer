{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5FPSD1kTshrjF2x2Ap22+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuralNetworkNecromancer/Alrik-Abenteurer/blob/main/Rulebook_chunk_automation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####INITIATE####\n",
        "!pip install qdrant-client openai google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests PyPDF2 pytz\n",
        "\n",
        "# utils\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from google.colab import drive, files, auth\n",
        "import pytz\n",
        "\n",
        "#data loading packages\n",
        "from google.colab import drive, auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import PyPDF2\n",
        "import re\n",
        "from google.colab import userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify folder and file details\n",
        "folder_dir = f'/content/drive/MyDrive/<your file path>'\n",
        "app_dir = f'{folder_dir}/App'\n",
        "targeted_folder = f'{folder_dir}/Rules'\n",
        "\n",
        "\n",
        "# Setup Openai\n",
        "OPENAI_API_KEY = userdata.get('openai')\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# OAuth2.0 Authentication\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# If modifying these SCOPES, delete the file token.json.\n",
        "creds = None\n",
        "if os.path.exists('token.json'):\n",
        "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = google.auth.default()\n",
        "\n",
        "# Build the Drive and Docs services\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "# Build the Google Sheets API client.\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "3zg74TZcoz4i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It seperates files by headers into single docs for upsertion. It could be used as a foundation to build an automation that chunks rulebooks in a meaningful way. However it still is uncomplete as it carries over some letters and doesnt properly process the last page (index related issues)\n",
        "\n",
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "\n",
        "# Function to calculate the indices for each page based on font size 14 titles\n",
        "def calculate_page_indices(doc, titles):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_indices = []\n",
        "    current_start_index = None\n",
        "    current_page = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        if font_size == 14 and not in_page:\n",
        "            if current_start_index is not None:\n",
        "                page_indices.append({'start': current_start_index, 'end': element.get('startIndex')})\n",
        "            current_start_index = element.get('startIndex')\n",
        "            current_page += 1\n",
        "            in_page = True\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    if in_page:\n",
        "        page_indices.append({'start': current_start_index, 'end': len(doc['body']['content'])})\n",
        "\n",
        "    for title_info in titles:\n",
        "        page_number = title_info['page']\n",
        "        if page_number <= len(page_indices):\n",
        "            title_info['indices'] = page_indices[page_number - 1]\n",
        "            title_info['pages_before'] = page_number - 1\n",
        "            title_info['pages_after'] = len(page_indices) - page_number\n",
        "\n",
        "    return titles\n",
        "\n",
        "def list_folders_and_docs_in_directory(directory_path):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        if folder['name'] == \"\":\n",
        "            docs_results = drive_service.files().list(\n",
        "                q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "                fields=\"files(id, name)\"\n",
        "            ).execute()\n",
        "\n",
        "            for doc_item in docs_results.get('files', []):\n",
        "                doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "                titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "                # Process each document for creating one-pagers\n",
        "                create_one_pagers_for_titles(doc_item.get('id'), titles)\n",
        "\n",
        "    return folder_names\n",
        "\n",
        "def calculate_total_content_length(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    total_length = 0\n",
        "    image_value = 1  # Assign 1 for each image\n",
        "\n",
        "    for element in content:\n",
        "        # Check for text in paragraphs\n",
        "        if 'paragraph' in element:\n",
        "            elements = element.get('paragraph', {}).get('elements', [])\n",
        "            for elem in elements:\n",
        "                if 'textRun' in elem:\n",
        "                    text_run = elem.get('textRun', {})\n",
        "                    text = text_run.get('content', '')\n",
        "                    total_length += len(text)\n",
        "                elif 'inlineObjectElement' in elem:\n",
        "                    # Count each image as a character\n",
        "                    total_length += image_value\n",
        "\n",
        "        # Additional elements like tables, etc., can be handled here\n",
        "\n",
        "    return total_length\n",
        "\n",
        "# Use this function within create_one_pagers_for_titles to get the total length\n",
        "def create_one_pagers_for_titles(document_id, titles):\n",
        "    original_doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    original_content_length = calculate_total_content_length(original_doc)\n",
        "\n",
        "    # Calculate the page indices for each title\n",
        "    titles_with_indices = calculate_page_indices(original_doc, titles)\n",
        "\n",
        "    for title_info in titles_with_indices:\n",
        "        indices = title_info.get('indices', {})\n",
        "        pages_before = title_info.get('pages_before')\n",
        "        pages_after = title_info.get('pages_after') + 1\n",
        "\n",
        "        # Print the indices for debugging\n",
        "        print(f\"Title: {title_info['title']}, Start Index: {indices.get('start')}, End Index: {indices.get('end')}, Pages Before: {pages_before}, Pages After: {pages_after}, Original Content Length: {original_content_length}\")\n",
        "\n",
        "        # Copy the original document\n",
        "        copied_doc = drive_service.files().copy(fileId=document_id, body={\"name\": title_info['title']}).execute()\n",
        "        copied_doc_id = copied_doc['id']\n",
        "\n",
        "        # Delete content after the desired page\n",
        "        if indices.get('end') and indices.get('end') < original_content_length:\n",
        "            docs_service.documents().batchUpdate(\n",
        "                documentId=copied_doc_id,\n",
        "                body={\n",
        "                    \"requests\": [\n",
        "                        {\"deleteContentRange\": {\"range\": {\"startIndex\": indices['end'], \"endIndex\": original_content_length}}}\n",
        "                    ]\n",
        "                }\n",
        "            ).execute()\n",
        "            print(f\"Deleted after the page: Start Index: {indices['end']}, End Index: {original_content_length}\")\n",
        "\n",
        "        # Delete content before the desired page\n",
        "        if indices.get('start') and indices.get('start') > 1:\n",
        "            docs_service.documents().batchUpdate(\n",
        "                documentId=copied_doc_id,\n",
        "                body={\n",
        "                    \"requests\": [\n",
        "                        {\"deleteContentRange\": {\"range\": {\"startIndex\": 1, \"endIndex\": indices['start']}}}\n",
        "                    ]\n",
        "                }\n",
        "            ).execute()\n",
        "            print(f\"Deleted before the page: Start Index: 1, End Index: {indices['start']}\")\n",
        "\n",
        "        print(f\"Processed {title_info['title']}\")\n",
        "\n",
        "# Example usage\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)"
      ],
      "metadata": {
        "id": "M5TPs6gD7bH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper to understand the total size of a certain doc\n",
        "document_id = \"15qEieVtUrJ8SzNLqMOAoFUQSt3JuRoFEDgAIziX_dJQ\"\n",
        "\n",
        "def calculate_total_content_length(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    total_length = 0\n",
        "    image_value = 1  # Assign 1 for each image\n",
        "\n",
        "    for element in content:\n",
        "        # Check for text in paragraphs\n",
        "        if 'paragraph' in element:\n",
        "            elements = element.get('paragraph', {}).get('elements', [])\n",
        "            for elem in elements:\n",
        "                if 'textRun' in elem:\n",
        "                    text_run = elem.get('textRun', {})\n",
        "                    text = text_run.get('content', '')\n",
        "                    total_length += len(text)\n",
        "                elif 'inlineObjectElement' in elem:\n",
        "                    # Count each image as a character\n",
        "                    total_length += image_value\n",
        "\n",
        "        # Additional elements like tables, etc., can be handled here\n",
        "\n",
        "    return total_length\n",
        "\n",
        "original_doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "original_content_length = calculate_total_content_length(original_doc)\n",
        "print(f\"Length: {original_content_length}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2DfQkgybSUQ",
        "outputId": "5a1c6c36-5186-4348-8dfb-f2d0599c1adc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 17537\n"
          ]
        }
      ]
    }
  ]
}