{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEmNrwRVJ6s5BmNu0TNzXW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####INITIATE####\n",
        "!pip install qdrant-client openai google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests PyPDF2 pytz\n",
        "\n",
        "# utils\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from google.colab import drive, files, auth\n",
        "import pytz\n",
        "\n",
        "#data loading packages\n",
        "from google.colab import drive, auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify folder and file details\n",
        "folder_dir = f'/content/drive/MyDrive/DSA/Alrik Abenteurer'\n",
        "app_dir = f'{folder_dir}/App'\n",
        "targeted_folder = f'{folder_dir}/Rules'\n",
        "company_name = 'blutmond'\n",
        "\n",
        "# Setup Openai\n",
        "OPENAI_API_KEY = \"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\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# OAuth2.0 Authentication\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# If modifying these SCOPES, delete the file token.json.\n",
        "creds = None\n",
        "if os.path.exists('token.json'):\n",
        "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = google.auth.default()\n",
        "\n",
        "# Build the Drive and Docs services\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "# Build the Google Sheets API client.\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "3zg74TZcoz4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "# Define the new functionality within the function\n",
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "def create_one_pagers_for_titles(document_id, titles):\n",
        "    for title_info in titles:\n",
        "        # Copy the original document\n",
        "        copied_doc = drive_service.files().copy(fileId=document_id, body={\"name\": title_info['title']}).execute()\n",
        "        copied_doc_id = copied_doc['id']\n",
        "\n",
        "        # Fetch the content of the copied document\n",
        "        doc = docs_service.documents().get(documentId=copied_doc_id).execute()\n",
        "        content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "        # Identify start and end indexes to keep based on the title's page number\n",
        "        start_index, end_index = None, None\n",
        "        current_page = 0\n",
        "        in_page = False\n",
        "\n",
        "        for element in content:\n",
        "            text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "            style = text_run.get('textStyle', {})\n",
        "            font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "            # Identify page start and end\n",
        "            if font_size == 14 and not in_page:\n",
        "                current_page += 1\n",
        "                in_page = True\n",
        "                if current_page == title_info['page']:\n",
        "                    start_index = element.get('startIndex')\n",
        "            elif in_page and '\\n' in text_run.get('content', ''):\n",
        "                in_page = False\n",
        "                if current_page == title_info['page']:\n",
        "                    end_index = element.get('endIndex')\n",
        "                    break\n",
        "\n",
        "        # Delete content outside of start and end indexes\n",
        "        if start_index is not None and end_index is not None:\n",
        "            # Delete content after the desired page\n",
        "            if end_index < len(doc['body']['content']):\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": end_index,\n",
        "                                        \"endIndex\": len(doc['body']['content'])\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "            # Delete content before the desired page\n",
        "            if start_index > 1:\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": 1,\n",
        "                                        \"endIndex\": start_index\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "\n",
        "\n",
        "def list_folders_and_docs_in_directory(directory_path):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        if folder['name'] == \"Waffenloser Kampf\":\n",
        "            docs_results = drive_service.files().list(\n",
        "                q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "                fields=\"files(id, name)\"\n",
        "            ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                # Create one-pagers for the titles of the document\n",
        "                create_one_pagers_for_titles(doc_item.get('id'), titles)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "M5TPs6gD7bH7",
        "outputId": "17530d9f-19c0-4ee5-ff1b-7096c4900071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36m<cell line: 264>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfolder_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;31m# Call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m \u001b[0mfolder_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_folders_and_docs_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargeted_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36mlist_folders_and_docs_in_directory\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# Create one-pagers for the titles of the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mcreate_one_pagers_for_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36mcreate_one_pagers_for_titles\u001b[0;34m(document_id, titles)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtitle_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Copy the original document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mcopied_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcopied_doc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopied_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# Handle retries for server-side errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         resp, content = _retry_request(\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mnum_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google_auth_httplib2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Make the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         response, content = self.http.request(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                     (response, content) = self._request(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                         \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthority\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachekey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_conn_request\u001b[0;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0;31m# If we get a BadStatusLine on the first try then that means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_folders_and_docs_in_directory(directory_path, limit_to_title=\"Beinarbeit\"):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        # Skip processing for the folder named \"Basis\"\n",
        "        if folder['name'] == \"Basis\":\n",
        "            continue\n",
        "\n",
        "        print(f\"Folder: {folder['name']}\")\n",
        "\n",
        "        docs_results = drive_service.files().list(\n",
        "            q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "            fields=\"files(id, name)\"\n",
        "        ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                if not limit_to_title or title_info['title'] == limit_to_title:\n",
        "                    create_document_from_title(doc_item.get('id'), title_info, parent_folder_id)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        folder_names.append(folder['name'])\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)"
      ],
      "metadata": {
        "id": "zvfFFzqBDd3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####LOAD DATA####\n",
        "\n",
        "# The ID and range of the spreadsheet.\n",
        "SPREADSHEET_ID = '1E5DouVxsAZyVT8AbDgMMba8FqEBWlwuSpJPU8ef1N4g'\n",
        "RANGE_NAME = 'mapping'  # e.g., 'Sheet1'\n",
        "\n",
        "# Call the Sheets API\n",
        "sheet = sheets_service.spreadsheets().values().get(\n",
        "    spreadsheetId=SPREADSHEET_ID,\n",
        "    range=RANGE_NAME\n",
        ").execute()\n",
        "\n",
        "# Get values and convert them into a dictionary.\n",
        "values = sheet.get('values', [])\n",
        "abbr_mapping = {row[0]: {'full_name': row[1], 'link': row[2]} for row in values[1:]}  # Excluding header row\n",
        "\n",
        "\n",
        "# Fetch the folder ID\n",
        "folder_id = None\n",
        "for folder_name in targeted_folders.split('/'):\n",
        "    query = f\"name='{folder_name}'\"\n",
        "    if folder_id is not None:\n",
        "        query += f\" and '{folder_id}' in parents\"\n",
        "    results = drive_service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, mimeType)\"\n",
        "    ).execute()\n",
        "\n",
        "    print(f\"Searching for folder: {folder_name}\")  # Print the folder being searched for\n",
        "    print(f\"Query used: {query}\")  # Print the query being used\n",
        "    print(f\"Query results: {results}\")  # Print results\n",
        "\n",
        "    items = results.get('files', [])\n",
        "    folder_found = False  # Flag to check if folder is found\n",
        "    for item in items:\n",
        "        if item['mimeType'] == 'application/vnd.google-apps.folder':  # Check if item is a folder\n",
        "            folder_id = item['id']\n",
        "            print(f\"Folder {folder_name} found with ID: {folder_id}\\n\")  # Print the found folder id\n",
        "            folder_found = True  # Update the flag\n",
        "            break  # Exit the loop once folder is found\n",
        "\n",
        "    if not folder_found:  # Check if folder was not found\n",
        "        print(f\"Folder {folder_name} not found.\\n\")\n",
        "        break\n",
        "\n",
        "def extract_info_from_title(title):\n",
        "    \"\"\"\n",
        "    Extract information from the title in the format:\n",
        "    <{Title Name} {Abbr} {Page Number} {f (optional)}>\n",
        "\n",
        "    Returns a dictionary with keys 'name', 'abbreviation', 'page_number', and 'continued'.\n",
        "    \"\"\"\n",
        "\n",
        "    info_pattern = re.compile(r'(?P<name>.*?) (?P<abbr>\\w+) (?P<page>\\d+)(?: (?P<continued>f))?')\n",
        "    match = info_pattern.search(title)\n",
        "    if match:\n",
        "        return {\n",
        "            \"name\": match.group(\"name\").strip(),\n",
        "            \"abbreviation\": match.group(\"abbr\"),\n",
        "            \"page_number\": int(match.group(\"page\")),\n",
        "            \"continued\": bool(match.group(\"continued\")),\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"name\": None,\n",
        "            \"abbreviation\": None,\n",
        "            \"page_number\": None,\n",
        "            \"continued\": False,\n",
        "        }\n",
        "\n",
        "# Fetch Google Docs files in the folder\n",
        "if folder_id:\n",
        "    results = drive_service.files().list(\n",
        "        q=f\"'{folder_id}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "    items = results.get('files', [])\n",
        "    if items:\n",
        "        formatted_data = []\n",
        "        for item in items:\n",
        "            # Extract document content\n",
        "            doc = docs_service.documents().get(documentId=item.get('id')).execute()\n",
        "            content = []\n",
        "            for element in doc.get('body', {}).get('content', []):\n",
        "                content.extend([part.get('textRun', {}).get('content', '')\n",
        "                                for part in element.get('paragraph', {}).get('elements', [])])\n",
        "            content_text = \"\".join(content)\n",
        "\n",
        "            # Extract info from title\n",
        "            extracted_info = extract_info_from_title(item.get(\"name\"))\n",
        "            abbr = extracted_info[\"abbreviation\"]\n",
        "\n",
        "            # Store data\n",
        "            doc_data = {\n",
        "                \"id\": item.get(\"id\"),\n",
        "                \"url\": f\"https://docs.google.com/document/d/{item.get('id')}\",\n",
        "                \"content_name\": item.get(\"name\"),\n",
        "                \"content_text\": content_text,\n",
        "                \"abbreviation\": extracted_info[\"abbreviation\"],\n",
        "                \"page_number\": extracted_info[\"page_number\"],\n",
        "                \"name\": extracted_info[\"name\"],\n",
        "                \"continued\": extracted_info[\"continued\"],\n",
        "                \"source_book_name\": None,  # Placeholder\n",
        "                \"source_book_url\": None     # Placeholder\n",
        "            }\n",
        "\n",
        "            if abbr in abbr_mapping:\n",
        "                doc_data[\"source_book_name\"] = abbr_mapping[abbr][\"full_name\"]\n",
        "                doc_data[\"source_book_url\"] = abbr_mapping[abbr][\"link\"]\n",
        "\n",
        "            formatted_data.append(doc_data)\n",
        "\n",
        "        # Save to a JSON file\n",
        "        with open(f'{app_dir}/{company_name}_drive_data.json', 'w') as f:\n",
        "            json.dump(formatted_data, f, indent=4)\n",
        "    else:\n",
        "        print(\"No documents found in the folder.\")\n"
      ],
      "metadata": {
        "id": "WL2vVQEYGwzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85ef0b4-37d3-4c37-c3b3-53ebe8f8ad41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder  not found. Continuing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####CREATE EMBEDDINGS####\n",
        "import openai\n",
        "import hashlib\n",
        "\n",
        "# consistent means there is a risk of collusion when used for multiple customer datasets stored in the same environment. Solution is to add a customer unqiue value pre or post hashing\n",
        "def consistent_hash(s):\n",
        "    \"\"\"Hashes a string and returns a consistent integer.\"\"\"\n",
        "    # Get a SHA256 hash of the string\n",
        "    result = hashlib.sha256(s.encode()).hexdigest()\n",
        "    # Convert the first 8 characters of the hash to an integer\n",
        "    return int(result[:8], 16)  # Converts the hex to an integer\n",
        "\n",
        "# Load the combined data\n",
        "with open(f'{app_dir}/{company_name}_drive_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Function to create embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    return openai.Embedding.create(input=[text], engine=model)['data'][0]['embedding']\n",
        "\n",
        "# Create embeddings and prepare for upload\n",
        "embeddings_data = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"content_text\") or \"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not text:\n",
        "        print(f\"Skipped item with ID {item['id']} due to empty 'content_text'.\")\n",
        "        continue\n",
        "\n",
        "    # Check if ID is a number or alphanumeric and handle accordingly\n",
        "    try:\n",
        "        # Try converting it directly\n",
        "        item_id = int(float(item[\"id\"]))\n",
        "    except ValueError:\n",
        "        # If direct conversion fails, hash it\n",
        "        item_id = consistent_hash(item[\"id\"])\n",
        "\n",
        "   # Create a shallow copy of the item to avoid mutating the original data\n",
        "    item_payload = item.copy()\n",
        "    # Remove the content_text from the payload\n",
        "    # item_payload.pop(\"content_text\", None)\n",
        "\n",
        "    embedding = get_embedding(text)\n",
        "    embeddings_data.append({\n",
        "        \"id\": item_id,\n",
        "        \"vector\": embedding,\n",
        "        \"payload\": item_payload  # Add the payload directly here\n",
        "    })\n",
        "\n",
        "    print(f\"Successfully embedded item with original ID {item['id']} (hashed ID: {item_id}).\")\n",
        "\n",
        "# Save embeddings to JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'w') as file:\n",
        "    json.dump(embeddings_data, file, indent=4)\n",
        "\n",
        "print(f\"\\n\\n------------------------------------------------------------------\\n\\nSaved {len(embeddings_data)} embeddings to embeddings_data.json.\\n\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3n5owskB-ghf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66101467-76a4-43cc-e5ea-c6df5f6aa0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded item with original ID 1eT7KdX2L38y59Kcp74N5Lqq6-or5q--fZkpLHDjMdDU (hashed ID: 637546719).\n",
            "Successfully embedded item with original ID 1sAdLCsSb9iCdYVkfInu0sKAp-5nsMGFWIRbwqrNaLG0 (hashed ID: 3766656703).\n",
            "Successfully embedded item with original ID 1IUdFci9TpupTHpSa4Ps1Qye7HkmprU7rPwora5uHyDs (hashed ID: 3967910555).\n",
            "Successfully embedded item with original ID 1pP2wSb033Ti17t2hwhQZrMk7xyp6uElaxEEVHaa20-s (hashed ID: 3645950601).\n",
            "Successfully embedded item with original ID 1hThZtcprXGoSSgPmBQJB0f0dcifKW1pIE1xK_-ZsA1A (hashed ID: 1162029853).\n",
            "Successfully embedded item with original ID 1UYn4cfOmhUliSj1BJ4GXpyGkDoG4wBwtAZijq7XL-e0 (hashed ID: 1835658885).\n",
            "Successfully embedded item with original ID 1ptSqGjaF3y58I5nK8_TY-8BF3tpZRkJlU1W-L_QFxM0 (hashed ID: 2121841143).\n",
            "Successfully embedded item with original ID 1umyNXaD9DmgxqFcLQBewbBMztNNK5toIJ0E6hmqVX5Y (hashed ID: 2964584255).\n",
            "Successfully embedded item with original ID 1onyZyLEL1oqdSJDvp8VUw4PaI90rXLtG3DzUz6YFNFU (hashed ID: 1267867026).\n",
            "Successfully embedded item with original ID 10KDWDlHuHPyRfum2rRGqRC_9xhCnyts4nFegj5u02dE (hashed ID: 2376614174).\n",
            "Successfully embedded item with original ID 1s4QVB3lrTPMu2pwPUSEebAClzzx1untci2C6cw6cP_w (hashed ID: 3910209491).\n",
            "Successfully embedded item with original ID 12hCVGibwkY_3PJ-lBxVHiSNF0bJmmN-GVT1aAq8J1lU (hashed ID: 2595858590).\n",
            "Successfully embedded item with original ID 1sVUKRwT6Y-s6ARb2KYMbWV8VaFSOumHz-1SxtX2hJEA (hashed ID: 2018676503).\n",
            "Successfully embedded item with original ID 1XgbPl4Zw5V8eCWiJl1lY0m8_AOpLY_LivR-i7LU0e3o (hashed ID: 2069506007).\n",
            "Successfully embedded item with original ID 1xOebBYJhV4hT8OYI1CnZFaIXRvA8jTGYz8nj99O4fXI (hashed ID: 4266206567).\n",
            "Successfully embedded item with original ID 1iubIUZ30x1B_OjuKHzu82NOnJ-dBOZf0gbxY-oVtx8k (hashed ID: 2841469302).\n",
            "Successfully embedded item with original ID 1qgyqzAkdzAfKY0oFLjLeXnBarrno3xMfLh2ppWX1wbQ (hashed ID: 3048210063).\n",
            "Successfully embedded item with original ID 15vUJuvfrk4g3E_44JYh0T1jcyaCO1x3J-AkpSU69JCo (hashed ID: 2398679150).\n",
            "Successfully embedded item with original ID 1bCMU1jee53JIRrAuuu-92TkJazc86U9qK9NX7FfmdKU (hashed ID: 2479692533).\n",
            "Successfully embedded item with original ID 1MKI4lVDL-eiMsC_z4GN_4VYhEjTJAITtKhF0ICbrKf8 (hashed ID: 3874399046).\n",
            "Successfully embedded item with original ID 1uqQKUbSF-0CRKSNLJ3zKS6meWlNLu42td1DBPKHq70s (hashed ID: 3296562022).\n",
            "Successfully embedded item with original ID 171AO8HbfFcEpVlPmFqHzr1u2wZI9kiyHbUIrLbWFY2s (hashed ID: 3487401638).\n",
            "Successfully embedded item with original ID 1HhoJyOrmqHhbLY3ppRYtFAXhqdrqX17wBlLJDh3YsAU (hashed ID: 683533415).\n",
            "Successfully embedded item with original ID 1mxtE0B-kcgw_stmgWoJ0MbwXxS4YTykTAGYeWJLjVNc (hashed ID: 1656485959).\n",
            "Successfully embedded item with original ID 1zRIV7rqQ9DKNCcYXwGxmtMX65czt57j_sN6N4Z3zqrQ (hashed ID: 1695141481).\n",
            "Successfully embedded item with original ID 1tbtjTgqSaHVXZ0JnUk3JvzpYClZapqewYwFas4vam3I (hashed ID: 721864250).\n",
            "Successfully embedded item with original ID 17kASsKkXYDgoQIzrFeQh_WMZGiz8f_oV7xRlTYPE5fY (hashed ID: 2840575792).\n",
            "Successfully embedded item with original ID 1m-yxn9J4syNvP4xtesNrnQoYj1wh2OzgBUL93awinoU (hashed ID: 1217166335).\n",
            "Successfully embedded item with original ID 1LsQLu_nW4gK4Z-TdTY0kGQzDzdEUt8CqH_seccflESQ (hashed ID: 2411323939).\n",
            "\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Saved 29 embeddings to embeddings_data.json.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####UPLOAD TO VECTOR DB####\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "# Setup Qdrant\n",
        "QDRANT_API_KEY = \"_G7mfag9Rvs9lQUMudiFSv9-TEiIbCWmHYxSoV4QqvSNLHesgV85DQ\"\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://aee976f2-9b39-4735-b2f7-c83f40d74925.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Load the embeddings data from JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'r') as file:\n",
        "    embeddings_data = json.load(file)\n",
        "\n",
        "# Define the collection name you want to use\n",
        "collection_name = f\"{company_name}_embeddings\"\n",
        "\n",
        "# Fetch a list of all collections\n",
        "all_collections = qdrant_client.get_collections()\n",
        "\n",
        "# Check if the collection name exists in the list of collections\n",
        "if collection_name in all_collections:\n",
        "    # If it exists, delete the current collection\n",
        "    qdrant_client.delete_collection(collection_name=collection_name)\n",
        "    print(f\"Deleted existing collection '{collection_name}'.\")\n",
        "\n",
        "# Define vectors configuration for the new collection\n",
        "vectors_config = models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "\n",
        "# Create the new collection\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=vectors_config\n",
        ")\n",
        "print(f\"Created new collection '{collection_name}'.\")\n",
        "\n",
        "# Now, upsert your embeddings\n",
        "qdrant_client.upsert(points=embeddings_data, collection_name=collection_name)\n",
        "\n",
        "print(f\"Uploaded {len(embeddings_data)} embeddings to Qdrant in the '{collection_name}' collection.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FyaAxr4BaJr",
        "outputId": "ad59bb16-8e3f-4156-c9ca-b98c68463edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new collection 'blutmond_embeddings'.\n",
            "Uploaded 29 embeddings to Qdrant in the 'blutmond_embeddings' collection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_rules(document_id, creds):\n",
        "    \"\"\"\n",
        "    Accesses a Google Docs document with a specific ID and retrieves its content.\n",
        "\n",
        "    Parameters:\n",
        "    - document_id (str): The ID of the Google Docs document.\n",
        "    - creds (Credentials): The credentials used to access Google Drive API.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the Google Docs document.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    # Build the Docs services\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "    # Retrieve the document using its ID\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "\n",
        "    # Extract content from the document\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "\n",
        "    # Combine extracted content into a single string\n",
        "    content_text = \"\".join(content)\n",
        "\n",
        "    return content_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwwLCXz3-Bj7",
        "outputId": "ec9dd01d-0e8e-484b-b3c7-6e6877a56d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spielregeln in Krze\n",
            "Ein aventurischer Held verfgt ber einen bestimmten Satz von Spielwerten: Eigenschaften stellen die geistigen und krperlichen Grundlagen des Helden dar, die durch bung wenig verndert werden knnen, die aber bei fast allen Spielentscheidungen eine Rolle spielen. Talente sind erlernte Fertigkeiten des Helden in verschiedenen Bereichen wie Kampf, Naturkunde oder Handwerk; die Erfolgswahrscheinlichkeit eines Talenteinsatzes hngt von den Eigenschaften und dem erlernten Talentwert ab. Schlielich gibt es noch abgeleitete Werte wie den Attacke-Wert, die Lebensenergie oder die Magieresistenz, die sich ebenfalls aus den Eigenschaften bestimmen. Zauberkundige Helden sind in der Lage, Zaubersprche einzusetzen, was wie der Gebrauch von Talenten funktioniert und ein bestimmtes Ma an Astralenergie kostet, whrend Geweihte die Kraft ihrer Gtter meist in Form von Liturgien umsetzen, die die ihnen von den Gttern verliehene Karmaenergie verbrauchen. Alles, was Sie als Spieler bentigen, sind ein Charakterbogen fr die Werte Ihres Helden, wie Sie ihn in diesem Band als Kopiervorlage oder zum Download auf unserer Homepage  ww.ulisses-spiele.de finden, einige weitere Notizzettel, Stifte sowie einen zwanzig seitigen und einen sechsseitigen Wrfel. Wrfel, Rundungen, Charakterbogen Das Schwarze Auge verwendet zwanzigseitige Wrfel (als W20 abgekrzt) und normale sechsseitige Wrfel (W6). Wenn sich irgendwo die Bezeichnung W (ohne Zahlenangabe) findet, ist damit ebenfalls der W6 gemeint. Wenn wir von einem W3 reden, meinen wir einen Wurf mit dem W6, wobei 1 und 2 als 1 gezhlt werden, 3 und 4 als 2 und 5 und 6 als 3. Eine Angabe wie 3W6 bedeutet, dass Sie drei sechsseitige Wrfel rollen und die Wrfelergebnisse aufaddieren knnen; eine Angabe 2W20+10 bedeutet, dass Sie zwei zwanzigseitige Wrfel rollen, die Ergebnisse zusammenzhlen und zustzlich 10 Punkte addieren. Wenn nicht ausdrcklich anders erwhnt, wird bei allen Rechnungen echt gerundet, d.h. Werte bis n,49 werden ab-, Werte ab n,50 werden aufgerundet. Speziell bei Halbierungen heit dies, dass aufgerundet wird. Wie bereits oben beschrieben, steht auf einem Charakterbogen (auch Heldendokument genannt) alles, was Sie an Werten fr das Spiel brauchen. Eigenschaften und Eigenschaftsproben Es gibt acht (gute) Eigenschaften, die einen Helden charakterisieren: Mut (abgekrzt MU), Klugheit (KL), Intuition (IN), Charisma (CH), Fingerfertigkeit (FF), Gewandtheit (GE), Konstitution (KO) und Krperkraft (KK). Die ersten vier Eigenschaften (MU, KL, IN, CH) werden auch Geistige Eigenschaften genannt, die letzten vier (FF, GE, KO, KK) Krperliche Eigenschaften. Je hher ein Wert eines Helden, desto hher die Chance, dass er eine entsprechende Probe besteht. Eigenschaftswerte bewegen sich fr menschenhnliche Wesen in einem Rahmen blicherweise zwischen 1 und 21, wobei Werte zwischen 8 und 14 den Schwerpunkt bilden. Bei einer Eigenschaftsprobe wrfelt der Spieler mit dem 20-seitigen Wrfel (1W20); wenn das Resultat nicht hher ist als der Wert der auf die Probe gestellten Eigenschaft, ist die Probe gelungen. Der Spielleiter kann je nach Situation Proben erleichtern oder verschrfen. Eine Erschwerung um +3 bedeutet, dass der Spieler zu seinem Wrfelresultat 3 addieren muss und trotzdem unter seinem Eigenschaftswert bleiben, eine Erleichterung von 5 heit, dass er von seinem Wrfelwurf 5 Punkte abziehen darf. Fllt bei der Probe eine 20, gilt dies als Patzer und bedeutet ein besonders schlechtes Resultat, eine gewrfelte 1 wiederum steht fr ein besonders gutes Ergebnis. Talentwerte und Talentproben Fertigkeiten wie Klettern, Reiten, Heilen u.. bezeichnen wir als Talente; das Ma der Erfahrung, die ein Held in einem Talent besitzt, ist sein Talentwert (kurz: TaW). blicherweise bewegt sich der Talentwert in einem Rahmen von 0 bis 21 (mit entsprechenden Ausnahmen von 4 bis +30). In einer Talentprobe flieen die Eigenschaftswerte eines Helden und sein Talentwert zusammen. Um die Probe zu bestehen, legt der Held nacheinander drei Eigenschaftsproben auf die zum Talent gehrigen Eigenschaften ab. Falls der Held ber einen positiven Talentwert verfgt, kann er versuchen, mit diesen Talentpunkten (TaP) misslungene Proben auszugleichen. Der Held kann nicht mehr Punkte aus dem TaW-Vorrat nehmen, als darin enthalten sind. Wenn er also schon bei der ersten Eigenschaftsprobe alle Punkte des Talentwertes verbraucht hat, dann stehen ihm fr die zweite und dritte Eigenschaftsprobe keine Ausgleichspunkte mehr zur Verfgung. Wenn er aber zu einem spteren Zeitpunkt eine neue Probe auf das gleiche Talent ablegen muss, steht ihm wieder der vollstndige Vorrat zur Verfgung. Wie viele Talentpunkte ein Held bei der Probe nicht verbraucht hat (die sogenannten TaP*), bestimmt in vielen Fllen die Qualitt der Talentprobe. Auch Talentproben knnen mit Zuschlgen (Erschwernissen) oder Abzgen (Erleichterungen) versehen werden. In solchen Fllen wird der Talentwert des Helden vor dem Auswrfeln der Probe mit dem Zuschlag oder dem Abzug verrechnet. Dazu wird eine Erschwernis vom Talentwert subtrahiert, eine Erleichterung zum TaW addiert. Wenn ein Held in einem Talent einen negativen Talentwert aufweist oder sein TaW durch situationsabhngige Modifikatoren unter null fllt, so muss er diesen Betrag unter null als Erschwernis zu jedem der drei Wrfe der Probe addieren und darf dennoch den jeweiligen Eigenschaftswert nicht bertreffen, wenn die Probe gelingen soll. Ein negativer TaW fhrt also stets zu drei erschwerten Eigenschaftsproben.  \n",
            "\n",
            "Kampfsystem\n",
            "Die Grundelemente des DSA-Kampfsystems sind der Attacke- und der Paradewert (kurz: AT und PA), die sich aus den Grundwerten (den abgeleiteten Eigenschaften AT- und PA-Basiswert) und Talentwerten (beispielsweise in Sbel oder Schwerter) zusammensetzen und auf die ganz wie bei einer Eigenschaftsprobe  Proben mit einem einzigen W20 abgelegt werden: Der Angreifer wrfelt eine Probe auf seinen AT-Wert, der Verteidiger auf seinen PA-Wert, oder krzer: Der Angreifer wrfelt eine Attacke, der Verteidiger eine Parade. Gelingt die Attacke und misslingt die Parade des Gegners, so ist dem Angreifer ein Treffer gelungen, der nun eine bestimmte Anzahl von Trefferpunkten (TP) anrichtet, die von Waffe zu Waffe unterschiedlich sind. Von diesen Trefferpunkten darf der Verteidiger nun noch den Wert seines Rstungsschutzes (RS) subtrahieren und muss den verbleibenden Rest als Schadenspunkte (SP) von seiner Lebensenergie (LE) abziehen. Ein Held, dessen Lebensenergie auf 0 oder darunter sinkt, ist so gut wie tot; ein Held, dessen LE auf 5 Punkte oder weniger gefallen ist, ist kampfunfhig (und auch unfhig zu zaubern oder die meisten Talente anzuwenden) und kann sich nur mit Mhe bei Bewusstsein halten. Ein besonders heftiger Treffer kann Wunden anrichten, die die Werte des Opfers verschlechtern. Wann ein Kmpfer in einem Gefecht an der Reihe ist, entscheidet sein Initiative-Wert (INI), der zu Beginn des Kampfes aus dem INI-Basiswert und einigen Modifikatoren bestimmt wird. Ein Kampf gliedert sich in Kampfrunden, in denen ein Beteiligter blicherweise zwei Aktionen (nmlich eine AT und eine PA) zur Verfgung hat. Der Kmpfer mit der hheren INI beginnt den Kampf mit einer AT, auf die der Verteidiger mit einer PA reagiert; dann handelt der Kmpfer mit der nchsthheren INI, dann der mit der dritthchsten usf., bis alle Beteiligten eine Attacke ausfhren durften. Dann beginnt die nchste Kampfrunde, und der Kampf geht weiter, bis einer der Kmpfer aufgibt, tot oder kampfunfhig ist  oder bis sein Gegner freiwillig von ihm ablsst. Um seine Chancen im Kampf zu verbessern, stehen dem Helden bestimmte Manver zur Verfgung, die er durch das Erlernen von Sonderfertigkeiten erwerben kann. Im Fernkampf wrfelt der Schtze auf seinen Fernkampf-Wert, der sich aus dem Fernkampf-Basiswert und dem Talentwert in der entsprechenden Fernwaffen-Gattung (Bogen, Wurfspeer etc.) bestimmt. Die Wahrscheinlichkeit, ein Ziel zu treffen, hngt vor allem von der Entfernung ab. Ein Treffer mit einer Fernwaffe erzeugt ebenfalls Trefferpunkte (s.o). Lebenspunkte, Ausdauer, Astralpunkte und Karmapunkte Neben den Eigenschaften des Helden, seinen Talenten und seinen Kampfwerten verfgt jeder Held noch ber mindestens zwei weitere Werte, die seine krperliche Verfassung widerspiegeln: die Lebensenergie (LE; unterteilt in Lebenspunkte, LeP), die reprsentiert, ob ein Held kleinere Blessuren oder grere Wunden erlitten hat, und die anzeigt, ob er dem Leben oder dem Tode nher steht, und die Ausdauer (AU; untergliedert in Ausdauerpunkte, AuP), die anzeigt, wie sehr ein Held auer Puste ist und ob er noch weitere krperliche Anstrengungen unternehmen kann. Diese beiden Werte berechnen sich zu Spielbeginn aus den Eigenschaften. Zauberkundige Helden verfgen darber hinaus noch ber die Astralenergie (AE; unterteilt in Astralpunkte, AsP), Helden, die Geweihte einer der aventurischen Gottheiten sind, verfgen neben der Lebensenergie und der Ausdauer noch ber die sogenannte Karmaenergie (KE; unterteilt in Karmapunkte, KaP), ein gttliches, schwer fassbares Fluidum, mit dem die Geweihten Wunder wirken und so den gttlichen Willen auf Dere reprsentieren knnen. Wenn ein Held durch Kmpfe Lebensenergie verloren hat, durch Anstrengung Ausdauerpunkte oder durch Zauber Astralenergie, sind diese Punkte nicht auf Dauer verloren, sondern regenerieren sich, whrend der Held ausruht. Karmaenergie kann durch Meditation zurckgewonnen Werden.\n",
            "\n",
            "Zauberei und Gtterwirken\n",
            "Zauberei begegnet den Aventuriern in Form von Zaubersprchen, magischen Ritualen und Artefakten und in der Person von Magiern, Elfen und anderen magisch begabten Personen wie Hexen oder Druiden und magischen Lebewesen wie Drachen und Einhrnern. Damit ein Lebewesen in Aventurien Zauberei anwenden kann, muss es von Geburt an dazu begabt sein, Astralenergie in sich aufzunehmen. Mit einem Zauberspruch wird diese Energie in einen magischen Effekt nach dem Willen des Zauberers umgewandelt; jeder Zauberspruch bentigt also gewisse Menge Astralenergie. Um einen Zauberspruch zu wirken, bentigt ein Magieanwender Konzentration: Dazu legt er eine Zauber-Probe ab, die wie eine Talentprobe gehandhabt wird. Fr diese Probe steht dem Zauberer je nach Spruch eine bestimmte Menge an Zauberfertigkeitspunkten (ZfP) zur Verfgung. Wird der Zauber gegen eine Person gewirkt, so gilt deren Magieresistenz (MR) als Erschwernis. Magische Rituale sind meist mit hherem Zeitaufwand durchzufhrende Zaubersprche, die dafr aber auch meist eine lngerfristige Wirkung zeigen, zum Beispiel die Beschwrung von Dmonen oder die Verzauberung von Gegenstnden zu magischen Artefakten. Letztere stellen gewissermaen eine Form von gespeicherter Zauberei dar und sind nicht selten auch fr weltliche, das heit: nicht mit arkanen Krften gesegnete Personen verwendbar. Aventurische Geweihte erhalten von ihren Gttern Karmaenergie verliehen, die sie in Form von Mirakeln oder Liturgien einsetzen knnen. Mirakel verbessern bestimmte Talentwerte des Helden, whrend Liturgien hnlich wie Zauber eine bernatrliche Wirkung hervorrufen. Fr beide Effekte ist eine sogenannte Mirakelprobe ntig, die wie eine Talentprobe gehandhabt wird, wobei der Liturgiekenntniswert (LkW) den Talentwert ersetzt.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####SEARCH FILES####\n",
        "\n",
        "# Example usage:\n",
        "Frage = \"Wie funktioniert DSA\" #@param {type: \"string\" }\n",
        "Anzahl_zu_durchsuchender_Dokumente = \"5\" #@param {type: \"string\" }\n",
        "user_input = Frage\n",
        "\n",
        "oauth2_service = build('oauth2', 'v2', credentials=creds)\n",
        "\n",
        "# Get user info\n",
        "user_info = oauth2_service.userinfo().get().execute()\n",
        "user_email = user_info['email']\n",
        "\n",
        "def get_base_rules(document_id, creds):\n",
        "    \"\"\"\n",
        "    Accesses a Google Docs document with a specific ID and retrieves its content.\n",
        "\n",
        "    Parameters:\n",
        "    - document_id (str): The ID of the Google Docs document.\n",
        "    - creds (Credentials): The credentials used to access Google Drive API.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the Google Docs document.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    # Build the Docs services\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "    # Retrieve the document using its ID\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "\n",
        "    # Extract content from the document\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "\n",
        "    # Combine extracted content into a single string\n",
        "    content_text = \"\".join(content)\n",
        "\n",
        "    return content_text\n",
        "\n",
        "# Example usage of the get_document_content function\n",
        "document_id = '1gA3Xicio9d6vZklPQnFE97M_Ajil4vxqJClEQDR4j-8'\n",
        "base_rules = get_base_rules(document_id, creds)\n",
        "\n",
        "# Generate a timestamp\n",
        "def generate_timestamp(timezone=\"CET\"):\n",
        "    \"\"\"\n",
        "    Generate a timestamp in the specified timezone.\n",
        "\n",
        "    Parameters:\n",
        "    - timezone: str, the timezone in which the timestamp is generated.\n",
        "\n",
        "    Returns:\n",
        "    - str, the timestamp.\n",
        "    \"\"\"\n",
        "    tz = pytz.timezone(timezone)\n",
        "    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def query_text(user_input, model=\"text-embedding-ada-002\", top_k=Anzahl_zu_durchsuchender_Dokumente):\n",
        "    \"\"\"\n",
        "    Query the vector database based on user input text.\n",
        "\n",
        "    Parameters:\n",
        "    - user_input: str, text input from the user to query against the vector database.\n",
        "    - model: str, the name of the OpenAI model used for embedding creation.\n",
        "    - top_k: int, the number of closest matches to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - list of dict, containing the top_k most similar documents to the user input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for user input\n",
        "    user_embedding = openai.Embedding.create(\n",
        "        input=[user_input], engine=model)['data'][0]['embedding']\n",
        "\n",
        "    # Perform query to find nearest neighbors in the vector database\n",
        "    results = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        limit=top_k,  # Retrieve top_k most similar points\n",
        "        query_vector=user_embedding,\n",
        "        with_payload=True  # Get the payload data along with the results\n",
        "    )\n",
        "\n",
        "    # Extract and return useful information from the results\n",
        "    similar_docs = [\n",
        "        {\n",
        "            \"id\": result.id,\n",
        "            \"payload\": result.payload,\n",
        "            \"score\": result.score,\n",
        "        }\n",
        "        for result in results\n",
        "    ]\n",
        "\n",
        "    return similar_docs\n",
        "\n",
        "\n",
        "ranked_results = query_text(user_input)\n",
        "\n",
        "# Print or further process the results as needed\n",
        "print(json.dumps(ranked_results, indent=4))  # Pretty print the results with indentation\n",
        "\n",
        "# Extract text content from the payloads into a list\n",
        "text_contents = []\n",
        "titles = []\n",
        "scores = []\n",
        "for doc in ranked_results:\n",
        "    payload = doc.get('payload', {})\n",
        "    text_content = payload.get('content_text', '')  # Replace with actual key if different\n",
        "    title = payload.get('content_name', '')  # Replace with actual key if different\n",
        "    score = doc.get('score', 0)\n",
        "\n",
        "    text_contents.append(text_content)\n",
        "    titles.append(title)\n",
        "    scores.append(score)\n",
        "\n",
        "# Now text_contents is a list of the text content from the ranked_results\n",
        "print(\"Extracted text contents:\")\n",
        "print(text_contents)\n",
        "\n",
        "# Generate an answer based on a file\n",
        "def generate_answer(user_input, text_contents):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\n",
        "            f\"role\": \"system\", \"content\": # setup und high level context\n",
        "            f\"Du bist ein Regelexperte und Spielleiter (eng.: Dungeon Master) einer Pen-&-Paper-Rollenspiel Kampagne im Das Schwarze Auge (DSA) Regelsystem.\"\n",
        "            f\"Dies sind die Basis Regeln: {base_rules}\"\n",
        "            f\"Bitte beantworte die Fragen mit entsprechenden detaillierten Erklrungen wie die Regeln funktionieren und an zu wenden sind.\"\n",
        "            f\"Bleibe jedoch faktisch genau bei den Regeln und erfinde nichts dazu.\"\n",
        "            f\"Antworte kurz und bndig in drei Stzen.\"\n",
        "            },\n",
        "            {\n",
        "              \"role\": \"user\", \"content\": # prompt\n",
        "             f\"Beantworte die Frage: '{user_input}', basierent auf deinem Wissen und diesem Dokument: \\n{text_contents} und der Basis Regeln\"\n",
        "             }\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message[\"content\"].strip()\n",
        "    return answer\n",
        "\n",
        "answer = generate_answer(user_input, text_contents)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "display(Markdown(\"### Frage: \"))\n",
        "display(Markdown(f\"_{user_input}_\\n\"))\n",
        "display(Markdown(\"### Antwort: \"))\n",
        "display(Markdown(f\"_{' '.join(answer.split())}_\"))\n",
        "\n",
        "# Additional Payload display:\n",
        "ref_texts = []\n",
        "for doc, title in zip(ranked_results, titles):\n",
        "    payload = doc.get('payload', {})\n",
        "    name = payload.get(\"name\", \"\")\n",
        "    abbr = payload.get(\"abbreviation\", \"\")\n",
        "    page = payload.get(\"page_number\", \"\")\n",
        "    cont = payload.get(\"continued\", False)\n",
        "    url = payload.get(\"source_book_url\", \"\")\n",
        "\n",
        "    # Construct reference string with Markdown formatting for hyperlink\n",
        "    cont_text = \"ff\" if cont else \"\"\n",
        "    ref_text = f\"[{name}]({url}) ({abbr} S. {page}{cont_text})\"\n",
        "    ref_texts.append(ref_text)\n",
        "\n",
        "# Concatenate reference texts and display\n",
        "ref_str = \", \".join(ref_texts)\n",
        "display(Markdown(\"### Regel Referenzen zum Nachschlagen: \"))\n",
        "display(Markdown(ref_str))\n",
        "\n",
        "# Load the existing QA pairs from the JSON file if it exists\n",
        "try:\n",
        "    with open(f'{app_dir}/{company_name}_qa_history.json', 'r') as file:\n",
        "        qa_history = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    # If file doesn't exist, initialize an empty list\n",
        "    qa_history = []\n",
        "\n",
        "# Append the new QA pair and save to file\n",
        "qa_pair = {\"user\": user_email,\n",
        "           \"timestamp\": generate_timestamp(),\n",
        "           \"question\": Frage,\n",
        "           \"answer\": answer,\n",
        "           \"returned_amount\": Anzahl_zu_durchsuchender_Dokumente,\n",
        "           \"references\": [{\"Titel\": t, \"Score\": s} for t, s in zip(titles, scores)]\n",
        "           }\n",
        "qa_history.append(qa_pair)\n",
        "\n",
        "# Save the updated QA pairs back to the JSON file\n",
        "with open(f'{app_dir}/{company_name}_qa_history.json', 'w') as file:\n",
        "    json.dump(qa_history, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Confirming the action\n",
        "display(Markdown(\"<sub>Logs gespeichert.</sub>\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "iinyoB8K1ws3",
        "outputId": "68124d33-57ac-492c-910f-bcb7a9057a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Frage: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Wie funktioniert DSA_\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Antwort: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_\"Das Schwarze Auge\" ist ein pen and paper Rollenspiel, in dem Spieler Charaktere in einer fiktiven Welt spielen. Jeder Charakter hat spezifische Eigenschaften, Talente und abgeleitete Werte, die das Ergebnis von Aktionen bestimmen, welche durch Wrfelwrfe reprsentiert werden. Kmpfe werden durch Attacke- und Paradewerte geregelt, Magie durch Astralenergie und gttliche Aktionen durch Karmaenergie._"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Regel Referenzen zum Nachschlagen: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[Ausfall](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 277), [Defensiver Kampfstil](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Doppelangriff](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Standfest](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 277), [Beidhndiger Kampf I](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<sub>Logs gespeichert.</sub>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Em5HQ6kATbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}