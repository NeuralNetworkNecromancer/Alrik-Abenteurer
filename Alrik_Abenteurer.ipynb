{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFn/GvrA9Z5U0AW6Dsd2QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuralNetworkNecromancer/NeuralNetworkNecromancer/blob/main/Alrik_Abenteurer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####INITIATE####\n",
        "!pip install qdrant-client openai google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests PyPDF2 pytz\n",
        "\n",
        "# utils\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from google.colab import drive, files, auth\n",
        "import pytz\n",
        "\n",
        "#data loading packages\n",
        "from google.colab import drive, auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import PyPDF2\n",
        "import re\n",
        "from google.colab import userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify folder and file details\n",
        "folder_dir = f'/content/drive/MyDrive/DSA/Alrik Abenteurer'\n",
        "app_dir = f'{folder_dir}/App'\n",
        "targeted_folder = f'{folder_dir}/Rules'\n",
        "company_name = 'blutmond'\n",
        "\n",
        "# Setup Openai\n",
        "OPENAI_API_KEY = userdata.get('openai')\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# OAuth2.0 Authentication\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# If modifying these SCOPES, delete the file token.json.\n",
        "creds = None\n",
        "if os.path.exists('token.json'):\n",
        "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = google.auth.default()\n",
        "\n",
        "# Build the Drive and Docs services\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "# Build the Google Sheets API client.\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "3zg74TZcoz4i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "# Define the new functionality within the function\n",
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "def create_one_pagers_for_titles(document_id, titles):\n",
        "    for title_info in titles:\n",
        "        # Copy the original document\n",
        "        copied_doc = drive_service.files().copy(fileId=document_id, body={\"name\": title_info['title']}).execute()\n",
        "        copied_doc_id = copied_doc['id']\n",
        "\n",
        "        # Fetch the content of the copied document\n",
        "        doc = docs_service.documents().get(documentId=copied_doc_id).execute()\n",
        "        content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "        # Identify start and end indexes to keep based on the title's page number\n",
        "        start_index, end_index = None, None\n",
        "        current_page = 0\n",
        "        in_page = False\n",
        "\n",
        "        for element in content:\n",
        "            text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "            style = text_run.get('textStyle', {})\n",
        "            font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "            # Identify page start and end\n",
        "            if font_size == 14 and not in_page:\n",
        "                current_page += 1\n",
        "                in_page = True\n",
        "                if current_page == title_info['page']:\n",
        "                    start_index = element.get('startIndex')\n",
        "            elif in_page and '\\n' in text_run.get('content', ''):\n",
        "                in_page = False\n",
        "                if current_page == title_info['page']:\n",
        "                    end_index = element.get('endIndex')\n",
        "                    break\n",
        "\n",
        "        # Delete content outside of start and end indexes\n",
        "        if start_index is not None and end_index is not None:\n",
        "            # Delete content after the desired page\n",
        "            if end_index < len(doc['body']['content']):\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": end_index,\n",
        "                                        \"endIndex\": len(doc['body']['content'])\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "            # Delete content before the desired page\n",
        "            if start_index > 1:\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": 1,\n",
        "                                        \"endIndex\": start_index\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "\n",
        "\n",
        "def list_folders_and_docs_in_directory(directory_path):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        if folder['name'] == \"Waffenloser Kampf\":\n",
        "            docs_results = drive_service.files().list(\n",
        "                q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "                fields=\"files(id, name)\"\n",
        "            ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                # Create one-pagers for the titles of the document\n",
        "                create_one_pagers_for_titles(doc_item.get('id'), titles)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M5TPs6gD7bH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_folders_and_docs_in_directory(directory_path, limit_to_title=\"Beinarbeit\"):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        # Skip processing for the folder named \"Basis\"\n",
        "        if folder['name'] == \"Basis\":\n",
        "            continue\n",
        "\n",
        "        print(f\"Folder: {folder['name']}\")\n",
        "\n",
        "        docs_results = drive_service.files().list(\n",
        "            q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "            fields=\"files(id, name)\"\n",
        "        ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                if not limit_to_title or title_info['title'] == limit_to_title:\n",
        "                    create_document_from_title(doc_item.get('id'), title_info, parent_folder_id)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        folder_names.append(folder['name'])\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)"
      ],
      "metadata": {
        "id": "zvfFFzqBDd3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####LOAD DATA####\n",
        "\n",
        "# The ID and range of the spreadsheet.\n",
        "SPREADSHEET_ID = '1E5DouVxsAZyVT8AbDgMMba8FqEBWlwuSpJPU8ef1N4g'\n",
        "RANGE_NAME = 'mapping'  # e.g., 'Sheet1'\n",
        "\n",
        "# Call the Sheets API\n",
        "sheet = sheets_service.spreadsheets().values().get(\n",
        "    spreadsheetId=SPREADSHEET_ID,\n",
        "    range=RANGE_NAME\n",
        ").execute()\n",
        "\n",
        "# Get values and convert them into a dictionary.\n",
        "values = sheet.get('values', [])\n",
        "abbr_mapping = {row[0]: {'full_name': row[1], 'link': row[2]} for row in values[1:]}  # Excluding header row\n",
        "\n",
        "\n",
        "# Fetch the folder ID\n",
        "folder_id = None\n",
        "for folder_name in targeted_folders.split('/'):\n",
        "    query = f\"name='{folder_name}'\"\n",
        "    if folder_id is not None:\n",
        "        query += f\" and '{folder_id}' in parents\"\n",
        "    results = drive_service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, mimeType)\"\n",
        "    ).execute()\n",
        "\n",
        "    print(f\"Searching for folder: {folder_name}\")  # Print the folder being searched for\n",
        "    print(f\"Query used: {query}\")  # Print the query being used\n",
        "    print(f\"Query results: {results}\")  # Print results\n",
        "\n",
        "    items = results.get('files', [])\n",
        "    folder_found = False  # Flag to check if folder is found\n",
        "    for item in items:\n",
        "        if item['mimeType'] == 'application/vnd.google-apps.folder':  # Check if item is a folder\n",
        "            folder_id = item['id']\n",
        "            print(f\"Folder {folder_name} found with ID: {folder_id}\\n\")  # Print the found folder id\n",
        "            folder_found = True  # Update the flag\n",
        "            break  # Exit the loop once folder is found\n",
        "\n",
        "    if not folder_found:  # Check if folder was not found\n",
        "        print(f\"Folder {folder_name} not found.\\n\")\n",
        "        break\n",
        "\n",
        "def extract_info_from_title(title):\n",
        "    \"\"\"\n",
        "    Extract information from the title in the format:\n",
        "    <{Title Name} {Abbr} {Page Number} {f (optional)}>\n",
        "\n",
        "    Returns a dictionary with keys 'name', 'abbreviation', 'page_number', and 'continued'.\n",
        "    \"\"\"\n",
        "\n",
        "    info_pattern = re.compile(r'(?P<name>.*?) (?P<abbr>\\w+) (?P<page>\\d+)(?: (?P<continued>f))?')\n",
        "    match = info_pattern.search(title)\n",
        "    if match:\n",
        "        return {\n",
        "            \"name\": match.group(\"name\").strip(),\n",
        "            \"abbreviation\": match.group(\"abbr\"),\n",
        "            \"page_number\": int(match.group(\"page\")),\n",
        "            \"continued\": bool(match.group(\"continued\")),\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"name\": None,\n",
        "            \"abbreviation\": None,\n",
        "            \"page_number\": None,\n",
        "            \"continued\": False,\n",
        "        }\n",
        "\n",
        "# Fetch Google Docs files in the folder\n",
        "if folder_id:\n",
        "    results = drive_service.files().list(\n",
        "        q=f\"'{folder_id}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "    items = results.get('files', [])\n",
        "    if items:\n",
        "        formatted_data = []\n",
        "        for item in items:\n",
        "            # Extract document content\n",
        "            doc = docs_service.documents().get(documentId=item.get('id')).execute()\n",
        "            content = []\n",
        "            for element in doc.get('body', {}).get('content', []):\n",
        "                content.extend([part.get('textRun', {}).get('content', '')\n",
        "                                for part in element.get('paragraph', {}).get('elements', [])])\n",
        "            content_text = \"\".join(content)\n",
        "\n",
        "            # Extract info from title\n",
        "            extracted_info = extract_info_from_title(item.get(\"name\"))\n",
        "            abbr = extracted_info[\"abbreviation\"]\n",
        "\n",
        "            # Store data\n",
        "            doc_data = {\n",
        "                \"id\": item.get(\"id\"),\n",
        "                \"url\": f\"https://docs.google.com/document/d/{item.get('id')}\",\n",
        "                \"content_name\": item.get(\"name\"),\n",
        "                \"content_text\": content_text,\n",
        "                \"abbreviation\": extracted_info[\"abbreviation\"],\n",
        "                \"page_number\": extracted_info[\"page_number\"],\n",
        "                \"name\": extracted_info[\"name\"],\n",
        "                \"continued\": extracted_info[\"continued\"],\n",
        "                \"source_book_name\": None,  # Placeholder\n",
        "                \"source_book_url\": None     # Placeholder\n",
        "            }\n",
        "\n",
        "            if abbr in abbr_mapping:\n",
        "                doc_data[\"source_book_name\"] = abbr_mapping[abbr][\"full_name\"]\n",
        "                doc_data[\"source_book_url\"] = abbr_mapping[abbr][\"link\"]\n",
        "\n",
        "            formatted_data.append(doc_data)\n",
        "\n",
        "        # Save to a JSON file\n",
        "        with open(f'{app_dir}/{company_name}_drive_data.json', 'w') as f:\n",
        "            json.dump(formatted_data, f, indent=4)\n",
        "    else:\n",
        "        print(\"No documents found in the folder.\")\n"
      ],
      "metadata": {
        "id": "WL2vVQEYGwzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####CREATE EMBEDDINGS####\n",
        "import openai\n",
        "import hashlib\n",
        "\n",
        "# consistent means there is a risk of collusion when used for multiple customer datasets stored in the same environment. Solution is to add a customer unqiue value pre or post hashing\n",
        "def consistent_hash(s):\n",
        "    \"\"\"Hashes a string and returns a consistent integer.\"\"\"\n",
        "    # Get a SHA256 hash of the string\n",
        "    result = hashlib.sha256(s.encode()).hexdigest()\n",
        "    # Convert the first 8 characters of the hash to an integer\n",
        "    return int(result[:8], 16)  # Converts the hex to an integer\n",
        "\n",
        "# Load the combined data\n",
        "with open(f'{app_dir}/{company_name}_drive_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Function to create embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    return openai.Embedding.create(input=[text], engine=model)['data'][0]['embedding']\n",
        "\n",
        "# Create embeddings and prepare for upload\n",
        "embeddings_data = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"content_text\") or \"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not text:\n",
        "        print(f\"Skipped item with ID {item['id']} due to empty 'content_text'.\")\n",
        "        continue\n",
        "\n",
        "    # Check if ID is a number or alphanumeric and handle accordingly\n",
        "    try:\n",
        "        # Try converting it directly\n",
        "        item_id = int(float(item[\"id\"]))\n",
        "    except ValueError:\n",
        "        # If direct conversion fails, hash it\n",
        "        item_id = consistent_hash(item[\"id\"])\n",
        "\n",
        "   # Create a shallow copy of the item to avoid mutating the original data\n",
        "    item_payload = item.copy()\n",
        "    # Remove the content_text from the payload\n",
        "    # item_payload.pop(\"content_text\", None)\n",
        "\n",
        "    embedding = get_embedding(text)\n",
        "    embeddings_data.append({\n",
        "        \"id\": item_id,\n",
        "        \"vector\": embedding,\n",
        "        \"payload\": item_payload  # Add the payload directly here\n",
        "    })\n",
        "\n",
        "    print(f\"Successfully embedded item with original ID {item['id']} (hashed ID: {item_id}).\")\n",
        "\n",
        "# Save embeddings to JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'w') as file:\n",
        "    json.dump(embeddings_data, file, indent=4)\n",
        "\n",
        "print(f\"\\n\\n------------------------------------------------------------------\\n\\nSaved {len(embeddings_data)} embeddings to embeddings_data.json.\\n\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3n5owskB-ghf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "# Setup Qdrant\n",
        "QDRANT_API_KEY = userdata.get('qdrant')\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=userdata.get('TBU_qdrant_url'),\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Load the embeddings data from JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'r') as file:\n",
        "    embeddings_data = json.load(file)\n",
        "\n",
        "# Define the collection name you want to use\n",
        "collection_name = f\"{company_name}_embeddings\"\n",
        "\n",
        "try:\n",
        "    # Fetch a list of all collections\n",
        "    all_collections = qdrant_client.get_collections()\n",
        "\n",
        "    # Check if the collection name exists in the list of collections\n",
        "    if collection_name in all_collections:\n",
        "        # If it exists, delete the current collection\n",
        "        qdrant_client.delete_collection(collection_name=collection_name)\n",
        "        print(f\"Deleted existing collection '{collection_name}'.\")\n",
        "    else:\n",
        "        print(f\"Collection '{collection_name}' does not exist. Creating new collection.\")\n",
        "\n",
        "    # Define vectors configuration for the new collection\n",
        "    vectors_config = models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "\n",
        "    # Create the new collection\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=vectors_config\n",
        "    )\n",
        "    print(f\"Created new collection '{collection_name}'.\")\n",
        "\n",
        "    # Now, upsert your embeddings\n",
        "    qdrant_client.upsert(points=embeddings_data, collection_name=collection_name)\n",
        "\n",
        "    print(f\"Uploaded {len(embeddings_data)} embeddings to Qdrant in the '{collection_name}' collection.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "7FyaAxr4BaJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####SEARCH FILES####\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "# Setup Qdrant\n",
        "QDRANT_API_KEY = userdata.get('qdrant')\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=userdata.get('TBU_qdrant_url'),\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Example usage:\n",
        "Frage = \"Wie macht man einen besonders starken Schlag\" #@param {type: \"string\" }\n",
        "Anzahl_zu_durchsuchender_Dokumente = \"5\" #@param {type: \"string\" }\n",
        "user_input = Frage\n",
        "\n",
        "oauth2_service = build('oauth2', 'v2', credentials=creds)\n",
        "\n",
        "# Get user info\n",
        "user_info = oauth2_service.userinfo().get().execute()\n",
        "user_email = user_info['email']\n",
        "\n",
        "def get_base_rules(document_id, creds):\n",
        "    \"\"\"\n",
        "    Accesses a Google Docs document with a specific ID and retrieves its content.\n",
        "\n",
        "    Parameters:\n",
        "    - document_id (str): The ID of the Google Docs document.\n",
        "    - creds (Credentials): The credentials used to access Google Drive API.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the Google Docs document.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    # Build the Docs services\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "    # Retrieve the document using its ID\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "\n",
        "    # Extract content from the document\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "\n",
        "    # Combine extracted content into a single string\n",
        "    content_text = \"\".join(content)\n",
        "\n",
        "    return content_text\n",
        "\n",
        "# Example usage of the get_document_content function\n",
        "document_id = '1gA3Xicio9d6vZklPQnFE97M_Ajil4vxqJClEQDR4j-8'\n",
        "base_rules = get_base_rules(document_id, creds)\n",
        "\n",
        "# Generate a timestamp\n",
        "def generate_timestamp(timezone=\"CET\"):\n",
        "    \"\"\"\n",
        "    Generate a timestamp in the specified timezone.\n",
        "\n",
        "    Parameters:\n",
        "    - timezone: str, the timezone in which the timestamp is generated.\n",
        "\n",
        "    Returns:\n",
        "    - str, the timestamp.\n",
        "    \"\"\"\n",
        "    tz = pytz.timezone(timezone)\n",
        "    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "collection_name = f\"{company_name}_embeddings\"\n",
        "\n",
        "def query_text(user_input, model=\"text-embedding-ada-002\", top_k=Anzahl_zu_durchsuchender_Dokumente):\n",
        "    \"\"\"\n",
        "    Query the vector database based on user input text.\n",
        "\n",
        "    Parameters:\n",
        "    - user_input: str, text input from the user to query against the vector database.\n",
        "    - model: str, the name of the OpenAI model used for embedding creation.\n",
        "    - top_k: int, the number of closest matches to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - list of dict, containing the top_k most similar documents to the user input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for user input\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    embedding = client.embeddings.create(\n",
        "        input=[user_input], model=model)\n",
        "    user_embedding = embedding.data[0].embedding\n",
        "\n",
        "    # Perform query to find nearest neighbors in the vector database\n",
        "    results = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        limit=top_k,  # Retrieve top_k most similar points\n",
        "        query_vector=user_embedding,\n",
        "        with_payload=True  # Get the payload data along with the results\n",
        "    )\n",
        "\n",
        "    # Extract and return useful information from the results\n",
        "    similar_docs = [\n",
        "        {\n",
        "            \"id\": result.id,\n",
        "            \"payload\": result.payload,\n",
        "            \"score\": result.score,\n",
        "        }\n",
        "        for result in results\n",
        "    ]\n",
        "\n",
        "    return similar_docs\n",
        "\n",
        "\n",
        "ranked_results = query_text(user_input)\n",
        "\n",
        "# Print or further process the results as needed\n",
        "print(json.dumps(ranked_results, indent=4))  # Pretty print the results with indentation\n",
        "\n",
        "# Extract text content from the payloads into a list\n",
        "text_contents = []\n",
        "titles = []\n",
        "scores = []\n",
        "for doc in ranked_results:\n",
        "    payload = doc.get('payload', {})\n",
        "    text_content = payload.get('content_text', '')  # Replace with actual key if different\n",
        "    title = payload.get('content_name', '')  # Replace with actual key if different\n",
        "    score = doc.get('score', 0)\n",
        "\n",
        "    text_contents.append(text_content)\n",
        "    titles.append(title)\n",
        "    scores.append(score)\n",
        "\n",
        "# Now text_contents is a list of the text content from the ranked_results\n",
        "print(\"Extracted text contents:\")\n",
        "print(text_contents)\n",
        "\n",
        "# Generate an answer based on a file\n",
        "def generate_answer(user_input, text_contents):\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\n",
        "            f\"role\": \"system\", \"content\": # setup und high level context\n",
        "            f\"Du bist ein Regelexperte und Spielleiter (eng.: Dungeon Master) einer Pen-&-Paper-Rollenspiel Kampagne im Das Schwarze Auge (DSA) Regelsystem.\"\n",
        "            f\"Dies sind die Basis Regeln: {base_rules}\"\n",
        "            f\"Bitte beantworte die Fragen mit entsprechenden detaillierten Erklärungen wie die Regeln funktionieren und an zu wenden sind.\"\n",
        "            f\"Bleibe jedoch faktisch genau bei den Regeln und erfinde nichts dazu.\"\n",
        "            f\"Antworte kurz und bündig in drei Sätzen.\"\n",
        "            },\n",
        "            {\n",
        "              \"role\": \"user\", \"content\": # prompt\n",
        "             f\"Beantworte die Frage: '{user_input}', basierent auf deinem Wissen und diesem Dokument: \\n{text_contents} und der Basis Regeln\"\n",
        "             }\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "    return answer\n",
        "\n",
        "answer = generate_answer(user_input, text_contents)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "display(Markdown(\"### Frage: \"))\n",
        "display(Markdown(f\"_{user_input}_\\n\"))\n",
        "display(Markdown(\"### Antwort: \"))\n",
        "display(Markdown(f\"_{' '.join(answer.split())}_\"))\n",
        "\n",
        "# Additional Payload display:\n",
        "ref_texts = []\n",
        "for doc, title in zip(ranked_results, titles):\n",
        "    payload = doc.get('payload', {})\n",
        "    name = payload.get(\"name\", \"\")\n",
        "    abbr = payload.get(\"abbreviation\", \"\")\n",
        "    page = payload.get(\"page_number\", \"\")\n",
        "    cont = payload.get(\"continued\", False)\n",
        "    url = payload.get(\"source_book_url\", \"\")\n",
        "\n",
        "    # Construct reference string with Markdown formatting for hyperlink\n",
        "    cont_text = \"ff\" if cont else \"\"\n",
        "    ref_text = f\"[{name}]({url}) ({abbr} S. {page}{cont_text})\"\n",
        "    ref_texts.append(ref_text)\n",
        "\n",
        "# Concatenate reference texts and display\n",
        "ref_str = \", \".join(ref_texts)\n",
        "display(Markdown(\"### Regel Referenzen zum Nachschlagen: \"))\n",
        "display(Markdown(ref_str))\n",
        "\n",
        "# Load the existing QA pairs from the JSON file if it exists\n",
        "try:\n",
        "    with open(f'{app_dir}/{company_name}_qa_history.json', 'r') as file:\n",
        "        qa_history = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    # If file doesn't exist, initialize an empty list\n",
        "    qa_history = []\n",
        "\n",
        "# Append the new QA pair and save to file\n",
        "qa_pair = {\"user\": user_email,\n",
        "           \"timestamp\": generate_timestamp(),\n",
        "           \"question\": Frage,\n",
        "           \"answer\": answer,\n",
        "           \"returned_amount\": Anzahl_zu_durchsuchender_Dokumente,\n",
        "           \"references\": [{\"Titel\": t, \"Score\": s} for t, s in zip(titles, scores)]\n",
        "           }\n",
        "qa_history.append(qa_pair)\n",
        "\n",
        "# Save the updated QA pairs back to the JSON file\n",
        "with open(f'{app_dir}/{company_name}_qa_history.json', 'w') as file:\n",
        "    json.dump(qa_history, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Confirming the action\n",
        "display(Markdown(\"<sub>Logs gespeichert.</sub>\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "iinyoB8K1ws3",
        "outputId": "e171d517-00f4-4cee-9caf-3ed542278440"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Frage: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Wie macht man einen besonders starken Schlag_\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Antwort: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Um einen besonders starken Schlag durchzuführen, gibt es das Manöver \"Betäubungsschlag\", welches eine Kombination aus dem \"Stumpfen Schlag\" und dem \"Wuchtschlag\" ist. Dieses Manöver ist nur mit Kenntnis der Sonderfertigkeit \"Betäubungsschlag\" möglich und nur mit bestimmten Waffen und Talenten ausführbar. Der Schaden wird in TP(A) nachgehalten und kann einen Gegner sofort außer Gefecht setzen, wenn der Angriff einen AU-Verlust erzeugt, der die Wundschwelle des Opfers übersteigt._"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Regel Referenzen zum Nachschlagen: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[Betäubungsschlag](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Betäubungsschlag](https://drive.google.com/file/d/1656cCroqLF3fg9Sr0N0Z8oSTDFiuNBQj/view?usp=sharing) (WdS S. 61), [Befreiungsschlag](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Eisenhagel](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Doppelangriff](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<sub>Logs gespeichert.</sub>"
          },
          "metadata": {}
        }
      ]
    }
  ]
}