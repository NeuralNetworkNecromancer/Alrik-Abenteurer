{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEmNrwRVJ6s5BmNu0TNzXW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####INITIATE####\n",
        "!pip install qdrant-client openai google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests PyPDF2 pytz\n",
        "\n",
        "# utils\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from google.colab import drive, files, auth\n",
        "import pytz\n",
        "\n",
        "#data loading packages\n",
        "from google.colab import drive, auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify folder and file details\n",
        "folder_dir = f'/content/drive/MyDrive/DSA/Alrik Abenteurer'\n",
        "app_dir = f'{folder_dir}/App'\n",
        "targeted_folder = f'{folder_dir}/Rules'\n",
        "company_name = 'blutmond'\n",
        "\n",
        "# Setup Openai\n",
        "OPENAI_API_KEY = \"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\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# OAuth2.0 Authentication\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# If modifying these SCOPES, delete the file token.json.\n",
        "creds = None\n",
        "if os.path.exists('token.json'):\n",
        "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = google.auth.default()\n",
        "\n",
        "# Build the Drive and Docs services\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "# Build the Google Sheets API client.\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "3zg74TZcoz4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "# Define the new functionality within the function\n",
        "def get_titles_and_page_numbers(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "    titles = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Check for titles based on font size\n",
        "        if font_size == 14 and not in_page:\n",
        "            titles.append({\"title\": text_run.get('content', '').strip(), \"page\": page_count + 1})\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return titles, page_count\n",
        "\n",
        "def count_pages_by_font_size(doc):\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "    page_count = 0\n",
        "    in_page = False\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        style = text_run.get('textStyle', {})\n",
        "        font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "        # Start a new page when font size 14 is encountered\n",
        "        if font_size == 14 and not in_page:\n",
        "            in_page = True\n",
        "            page_count += 1\n",
        "        # End current page when a newline character is found after a font size 14 line\n",
        "        elif in_page and '\\n' in text_run.get('content', ''):\n",
        "            in_page = False\n",
        "\n",
        "    return page_count\n",
        "\n",
        "def google_doc_to_markdown(document_id):\n",
        "    # Fetch the document content\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "    content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "    markdown_content = []\n",
        "\n",
        "    for element in content:\n",
        "        text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "        text = text_run.get('content', '')\n",
        "        style = text_run.get('textStyle', {})\n",
        "\n",
        "        if 'bold' in style and style['bold']:\n",
        "            text = f\"**{text}**\"\n",
        "        if 'italic' in style and style['italic']:\n",
        "            text = f\"*{text}*\"\n",
        "\n",
        "        heading_style = element.get('paragraph', {}).get('paragraphStyle', {}).get('namedStyleType', None)\n",
        "        if heading_style and 'HEADING' in heading_style:\n",
        "            heading_level = int(heading_style.split('_')[-1])\n",
        "            text = f\"{'#' * heading_level} {text}\"\n",
        "\n",
        "        markdown_content.append(text)\n",
        "\n",
        "    return '\\n'.join(markdown_content)\n",
        "\n",
        "def create_one_pagers_for_titles(document_id, titles):\n",
        "    for title_info in titles:\n",
        "        # Copy the original document\n",
        "        copied_doc = drive_service.files().copy(fileId=document_id, body={\"name\": title_info['title']}).execute()\n",
        "        copied_doc_id = copied_doc['id']\n",
        "\n",
        "        # Fetch the content of the copied document\n",
        "        doc = docs_service.documents().get(documentId=copied_doc_id).execute()\n",
        "        content = doc.get('body', {}).get('content', [])\n",
        "\n",
        "        # Identify start and end indexes to keep based on the title's page number\n",
        "        start_index, end_index = None, None\n",
        "        current_page = 0\n",
        "        in_page = False\n",
        "\n",
        "        for element in content:\n",
        "            text_run = element.get('paragraph', {}).get('elements', [{}])[0].get('textRun', {})\n",
        "            style = text_run.get('textStyle', {})\n",
        "            font_size = style.get('fontSize', {}).get('magnitude', None)\n",
        "\n",
        "            # Identify page start and end\n",
        "            if font_size == 14 and not in_page:\n",
        "                current_page += 1\n",
        "                in_page = True\n",
        "                if current_page == title_info['page']:\n",
        "                    start_index = element.get('startIndex')\n",
        "            elif in_page and '\\n' in text_run.get('content', ''):\n",
        "                in_page = False\n",
        "                if current_page == title_info['page']:\n",
        "                    end_index = element.get('endIndex')\n",
        "                    break\n",
        "\n",
        "        # Delete content outside of start and end indexes\n",
        "        if start_index is not None and end_index is not None:\n",
        "            # Delete content after the desired page\n",
        "            if end_index < len(doc['body']['content']):\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": end_index,\n",
        "                                        \"endIndex\": len(doc['body']['content'])\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "            # Delete content before the desired page\n",
        "            if start_index > 1:\n",
        "                docs_service.documents().batchUpdate(\n",
        "                    documentId=copied_doc_id,\n",
        "                    body={\n",
        "                        \"requests\": [\n",
        "                            {\n",
        "                                \"deleteContentRange\": {\n",
        "                                    \"range\": {\n",
        "                                        \"startIndex\": 1,\n",
        "                                        \"endIndex\": start_index\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ).execute()\n",
        "\n",
        "\n",
        "\n",
        "def list_folders_and_docs_in_directory(directory_path):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        if folder['name'] == \"Waffenloser Kampf\":\n",
        "            docs_results = drive_service.files().list(\n",
        "                q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "                fields=\"files(id, name)\"\n",
        "            ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                # Create one-pagers for the titles of the document\n",
        "                create_one_pagers_for_titles(doc_item.get('id'), titles)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "M5TPs6gD7bH7",
        "outputId": "17530d9f-19c0-4ee5-ff1b-7096c4900071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36m<cell line: 264>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfolder_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;31m# Call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m \u001b[0mfolder_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_folders_and_docs_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargeted_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36mlist_folders_and_docs_in_directory\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# Create one-pagers for the titles of the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mcreate_one_pagers_for_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-dbf68e0db01a>\u001b[0m in \u001b[0;36mcreate_one_pagers_for_titles\u001b[0;34m(document_id, titles)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtitle_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Copy the original document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mcopied_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcopied_doc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopied_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# Handle retries for server-side errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         resp, content = _retry_request(\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mnum_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google_auth_httplib2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Make the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         response, content = self.http.request(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                     (response, content) = self._request(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                         \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthority\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachekey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_conn_request\u001b[0;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0;31m# If we get a BadStatusLine on the first try then that means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_folders_and_docs_in_directory(directory_path, limit_to_title=\"Beinarbeit\"):\n",
        "    folder_names = []\n",
        "    directory_parts = directory_path.split('/')\n",
        "    current_folder_id = None\n",
        "\n",
        "    for part in directory_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if current_folder_id:\n",
        "            query = f\"name='{part}' and '{current_folder_id}' in parents\"\n",
        "        else:\n",
        "            query = f\"name='{part}'\"\n",
        "\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id, name, mimeType)\").execute()\n",
        "        for item in results.get('files', []):\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                current_folder_id = item['id']\n",
        "                break\n",
        "\n",
        "    folder_results = drive_service.files().list(\n",
        "        q=f\"'{current_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "\n",
        "    for folder in folder_results.get('files', []):\n",
        "        # Skip processing for the folder named \"Basis\"\n",
        "        if folder['name'] == \"Basis\":\n",
        "            continue\n",
        "\n",
        "        print(f\"Folder: {folder['name']}\")\n",
        "\n",
        "        docs_results = drive_service.files().list(\n",
        "            q=f\"'{folder['id']}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "            fields=\"files(id, name)\"\n",
        "        ).execute()\n",
        "\n",
        "        for doc_item in docs_results.get('files', []):\n",
        "            doc = docs_service.documents().get(documentId=doc_item.get('id')).execute()\n",
        "            titles, number_of_pages = get_titles_and_page_numbers(doc)\n",
        "\n",
        "            # Fetch the parent folder ID of the document to save the new documents there\n",
        "            parent_folder_id = drive_service.files().get(fileId=doc_item.get('id'), fields=\"parents\").execute().get(\"parents\", [])[0]\n",
        "\n",
        "            for title_info in titles:\n",
        "                pages_before_title = title_info['page'] - 1\n",
        "                pages_after_title = number_of_pages - title_info['page']\n",
        "\n",
        "                if not limit_to_title or title_info['title'] == limit_to_title:\n",
        "                    create_document_from_title(doc_item.get('id'), title_info, parent_folder_id)\n",
        "\n",
        "\n",
        "                print(f\"    Title: {title_info['title']} - Page: {title_info['page']} - Pages Before: {pages_before_title} - Pages After: {pages_after_title}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        folder_names.append(folder['name'])\n",
        "\n",
        "    return folder_names\n",
        "# Call the function\n",
        "folder_names = list_folders_and_docs_in_directory(targeted_folder)"
      ],
      "metadata": {
        "id": "zvfFFzqBDd3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####LOAD DATA####\n",
        "\n",
        "# The ID and range of the spreadsheet.\n",
        "SPREADSHEET_ID = '1E5DouVxsAZyVT8AbDgMMba8FqEBWlwuSpJPU8ef1N4g'\n",
        "RANGE_NAME = 'mapping'  # e.g., 'Sheet1'\n",
        "\n",
        "# Call the Sheets API\n",
        "sheet = sheets_service.spreadsheets().values().get(\n",
        "    spreadsheetId=SPREADSHEET_ID,\n",
        "    range=RANGE_NAME\n",
        ").execute()\n",
        "\n",
        "# Get values and convert them into a dictionary.\n",
        "values = sheet.get('values', [])\n",
        "abbr_mapping = {row[0]: {'full_name': row[1], 'link': row[2]} for row in values[1:]}  # Excluding header row\n",
        "\n",
        "\n",
        "# Fetch the folder ID\n",
        "folder_id = None\n",
        "for folder_name in targeted_folders.split('/'):\n",
        "    query = f\"name='{folder_name}'\"\n",
        "    if folder_id is not None:\n",
        "        query += f\" and '{folder_id}' in parents\"\n",
        "    results = drive_service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, mimeType)\"\n",
        "    ).execute()\n",
        "\n",
        "    print(f\"Searching for folder: {folder_name}\")  # Print the folder being searched for\n",
        "    print(f\"Query used: {query}\")  # Print the query being used\n",
        "    print(f\"Query results: {results}\")  # Print results\n",
        "\n",
        "    items = results.get('files', [])\n",
        "    folder_found = False  # Flag to check if folder is found\n",
        "    for item in items:\n",
        "        if item['mimeType'] == 'application/vnd.google-apps.folder':  # Check if item is a folder\n",
        "            folder_id = item['id']\n",
        "            print(f\"Folder {folder_name} found with ID: {folder_id}\\n\")  # Print the found folder id\n",
        "            folder_found = True  # Update the flag\n",
        "            break  # Exit the loop once folder is found\n",
        "\n",
        "    if not folder_found:  # Check if folder was not found\n",
        "        print(f\"Folder {folder_name} not found.\\n\")\n",
        "        break\n",
        "\n",
        "def extract_info_from_title(title):\n",
        "    \"\"\"\n",
        "    Extract information from the title in the format:\n",
        "    <{Title Name} {Abbr} {Page Number} {f (optional)}>\n",
        "\n",
        "    Returns a dictionary with keys 'name', 'abbreviation', 'page_number', and 'continued'.\n",
        "    \"\"\"\n",
        "\n",
        "    info_pattern = re.compile(r'(?P<name>.*?) (?P<abbr>\\w+) (?P<page>\\d+)(?: (?P<continued>f))?')\n",
        "    match = info_pattern.search(title)\n",
        "    if match:\n",
        "        return {\n",
        "            \"name\": match.group(\"name\").strip(),\n",
        "            \"abbreviation\": match.group(\"abbr\"),\n",
        "            \"page_number\": int(match.group(\"page\")),\n",
        "            \"continued\": bool(match.group(\"continued\")),\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"name\": None,\n",
        "            \"abbreviation\": None,\n",
        "            \"page_number\": None,\n",
        "            \"continued\": False,\n",
        "        }\n",
        "\n",
        "# Fetch Google Docs files in the folder\n",
        "if folder_id:\n",
        "    results = drive_service.files().list(\n",
        "        q=f\"'{folder_id}' in parents and mimeType='application/vnd.google-apps.document'\",\n",
        "        fields=\"files(id, name)\"\n",
        "    ).execute()\n",
        "    items = results.get('files', [])\n",
        "    if items:\n",
        "        formatted_data = []\n",
        "        for item in items:\n",
        "            # Extract document content\n",
        "            doc = docs_service.documents().get(documentId=item.get('id')).execute()\n",
        "            content = []\n",
        "            for element in doc.get('body', {}).get('content', []):\n",
        "                content.extend([part.get('textRun', {}).get('content', '')\n",
        "                                for part in element.get('paragraph', {}).get('elements', [])])\n",
        "            content_text = \"\".join(content)\n",
        "\n",
        "            # Extract info from title\n",
        "            extracted_info = extract_info_from_title(item.get(\"name\"))\n",
        "            abbr = extracted_info[\"abbreviation\"]\n",
        "\n",
        "            # Store data\n",
        "            doc_data = {\n",
        "                \"id\": item.get(\"id\"),\n",
        "                \"url\": f\"https://docs.google.com/document/d/{item.get('id')}\",\n",
        "                \"content_name\": item.get(\"name\"),\n",
        "                \"content_text\": content_text,\n",
        "                \"abbreviation\": extracted_info[\"abbreviation\"],\n",
        "                \"page_number\": extracted_info[\"page_number\"],\n",
        "                \"name\": extracted_info[\"name\"],\n",
        "                \"continued\": extracted_info[\"continued\"],\n",
        "                \"source_book_name\": None,  # Placeholder\n",
        "                \"source_book_url\": None     # Placeholder\n",
        "            }\n",
        "\n",
        "            if abbr in abbr_mapping:\n",
        "                doc_data[\"source_book_name\"] = abbr_mapping[abbr][\"full_name\"]\n",
        "                doc_data[\"source_book_url\"] = abbr_mapping[abbr][\"link\"]\n",
        "\n",
        "            formatted_data.append(doc_data)\n",
        "\n",
        "        # Save to a JSON file\n",
        "        with open(f'{app_dir}/{company_name}_drive_data.json', 'w') as f:\n",
        "            json.dump(formatted_data, f, indent=4)\n",
        "    else:\n",
        "        print(\"No documents found in the folder.\")\n"
      ],
      "metadata": {
        "id": "WL2vVQEYGwzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85ef0b4-37d3-4c37-c3b3-53ebe8f8ad41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder  not found. Continuing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####CREATE EMBEDDINGS####\n",
        "import openai\n",
        "import hashlib\n",
        "\n",
        "# consistent means there is a risk of collusion when used for multiple customer datasets stored in the same environment. Solution is to add a customer unqiue value pre or post hashing\n",
        "def consistent_hash(s):\n",
        "    \"\"\"Hashes a string and returns a consistent integer.\"\"\"\n",
        "    # Get a SHA256 hash of the string\n",
        "    result = hashlib.sha256(s.encode()).hexdigest()\n",
        "    # Convert the first 8 characters of the hash to an integer\n",
        "    return int(result[:8], 16)  # Converts the hex to an integer\n",
        "\n",
        "# Load the combined data\n",
        "with open(f'{app_dir}/{company_name}_drive_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Function to create embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    return openai.Embedding.create(input=[text], engine=model)['data'][0]['embedding']\n",
        "\n",
        "# Create embeddings and prepare for upload\n",
        "embeddings_data = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"content_text\") or \"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not text:\n",
        "        print(f\"Skipped item with ID {item['id']} due to empty 'content_text'.\")\n",
        "        continue\n",
        "\n",
        "    # Check if ID is a number or alphanumeric and handle accordingly\n",
        "    try:\n",
        "        # Try converting it directly\n",
        "        item_id = int(float(item[\"id\"]))\n",
        "    except ValueError:\n",
        "        # If direct conversion fails, hash it\n",
        "        item_id = consistent_hash(item[\"id\"])\n",
        "\n",
        "   # Create a shallow copy of the item to avoid mutating the original data\n",
        "    item_payload = item.copy()\n",
        "    # Remove the content_text from the payload\n",
        "    # item_payload.pop(\"content_text\", None)\n",
        "\n",
        "    embedding = get_embedding(text)\n",
        "    embeddings_data.append({\n",
        "        \"id\": item_id,\n",
        "        \"vector\": embedding,\n",
        "        \"payload\": item_payload  # Add the payload directly here\n",
        "    })\n",
        "\n",
        "    print(f\"Successfully embedded item with original ID {item['id']} (hashed ID: {item_id}).\")\n",
        "\n",
        "# Save embeddings to JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'w') as file:\n",
        "    json.dump(embeddings_data, file, indent=4)\n",
        "\n",
        "print(f\"\\n\\n------------------------------------------------------------------\\n\\nSaved {len(embeddings_data)} embeddings to embeddings_data.json.\\n\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3n5owskB-ghf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66101467-76a4-43cc-e5ea-c6df5f6aa0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded item with original ID 1eT7KdX2L38y59Kcp74N5Lqq6-or5q--fZkpLHDjMdDU (hashed ID: 637546719).\n",
            "Successfully embedded item with original ID 1sAdLCsSb9iCdYVkfInu0sKAp-5nsMGFWIRbwqrNaLG0 (hashed ID: 3766656703).\n",
            "Successfully embedded item with original ID 1IUdFci9TpupTHpSa4Ps1Qye7HkmprU7rPwora5uHyDs (hashed ID: 3967910555).\n",
            "Successfully embedded item with original ID 1pP2wSb033Ti17t2hwhQZrMk7xyp6uElaxEEVHaa20-s (hashed ID: 3645950601).\n",
            "Successfully embedded item with original ID 1hThZtcprXGoSSgPmBQJB0f0dcifKW1pIE1xK_-ZsA1A (hashed ID: 1162029853).\n",
            "Successfully embedded item with original ID 1UYn4cfOmhUliSj1BJ4GXpyGkDoG4wBwtAZijq7XL-e0 (hashed ID: 1835658885).\n",
            "Successfully embedded item with original ID 1ptSqGjaF3y58I5nK8_TY-8BF3tpZRkJlU1W-L_QFxM0 (hashed ID: 2121841143).\n",
            "Successfully embedded item with original ID 1umyNXaD9DmgxqFcLQBewbBMztNNK5toIJ0E6hmqVX5Y (hashed ID: 2964584255).\n",
            "Successfully embedded item with original ID 1onyZyLEL1oqdSJDvp8VUw4PaI90rXLtG3DzUz6YFNFU (hashed ID: 1267867026).\n",
            "Successfully embedded item with original ID 10KDWDlHuHPyRfum2rRGqRC_9xhCnyts4nFegj5u02dE (hashed ID: 2376614174).\n",
            "Successfully embedded item with original ID 1s4QVB3lrTPMu2pwPUSEebAClzzx1untci2C6cw6cP_w (hashed ID: 3910209491).\n",
            "Successfully embedded item with original ID 12hCVGibwkY_3PJ-lBxVHiSNF0bJmmN-GVT1aAq8J1lU (hashed ID: 2595858590).\n",
            "Successfully embedded item with original ID 1sVUKRwT6Y-s6ARb2KYMbWV8VaFSOumHz-1SxtX2hJEA (hashed ID: 2018676503).\n",
            "Successfully embedded item with original ID 1XgbPl4Zw5V8eCWiJl1lY0m8_AOpLY_LivR-i7LU0e3o (hashed ID: 2069506007).\n",
            "Successfully embedded item with original ID 1xOebBYJhV4hT8OYI1CnZFaIXRvA8jTGYz8nj99O4fXI (hashed ID: 4266206567).\n",
            "Successfully embedded item with original ID 1iubIUZ30x1B_OjuKHzu82NOnJ-dBOZf0gbxY-oVtx8k (hashed ID: 2841469302).\n",
            "Successfully embedded item with original ID 1qgyqzAkdzAfKY0oFLjLeXnBarrno3xMfLh2ppWX1wbQ (hashed ID: 3048210063).\n",
            "Successfully embedded item with original ID 15vUJuvfrk4g3E_44JYh0T1jcyaCO1x3J-AkpSU69JCo (hashed ID: 2398679150).\n",
            "Successfully embedded item with original ID 1bCMU1jee53JIRrAuuu-92TkJazc86U9qK9NX7FfmdKU (hashed ID: 2479692533).\n",
            "Successfully embedded item with original ID 1MKI4lVDL-eiMsC_z4GN_4VYhEjTJAITtKhF0ICbrKf8 (hashed ID: 3874399046).\n",
            "Successfully embedded item with original ID 1uqQKUbSF-0CRKSNLJ3zKS6meWlNLu42td1DBPKHq70s (hashed ID: 3296562022).\n",
            "Successfully embedded item with original ID 171AO8HbfFcEpVlPmFqHzr1u2wZI9kiyHbUIrLbWFY2s (hashed ID: 3487401638).\n",
            "Successfully embedded item with original ID 1HhoJyOrmqHhbLY3ppRYtFAXhqdrqX17wBlLJDh3YsAU (hashed ID: 683533415).\n",
            "Successfully embedded item with original ID 1mxtE0B-kcgw_stmgWoJ0MbwXxS4YTykTAGYeWJLjVNc (hashed ID: 1656485959).\n",
            "Successfully embedded item with original ID 1zRIV7rqQ9DKNCcYXwGxmtMX65czt57j_sN6N4Z3zqrQ (hashed ID: 1695141481).\n",
            "Successfully embedded item with original ID 1tbtjTgqSaHVXZ0JnUk3JvzpYClZapqewYwFas4vam3I (hashed ID: 721864250).\n",
            "Successfully embedded item with original ID 17kASsKkXYDgoQIzrFeQh_WMZGiz8f_oV7xRlTYPE5fY (hashed ID: 2840575792).\n",
            "Successfully embedded item with original ID 1m-yxn9J4syNvP4xtesNrnQoYj1wh2OzgBUL93awinoU (hashed ID: 1217166335).\n",
            "Successfully embedded item with original ID 1LsQLu_nW4gK4Z-TdTY0kGQzDzdEUt8CqH_seccflESQ (hashed ID: 2411323939).\n",
            "\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Saved 29 embeddings to embeddings_data.json.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####UPLOAD TO VECTOR DB####\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "# Setup Qdrant\n",
        "QDRANT_API_KEY = \"_G7mfag9Rvs9lQUMudiFSv9-TEiIbCWmHYxSoV4QqvSNLHesgV85DQ\"\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://aee976f2-9b39-4735-b2f7-c83f40d74925.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Load the embeddings data from JSON file\n",
        "with open(f'{app_dir}/{company_name}_embeddings_data.json', 'r') as file:\n",
        "    embeddings_data = json.load(file)\n",
        "\n",
        "# Define the collection name you want to use\n",
        "collection_name = f\"{company_name}_embeddings\"\n",
        "\n",
        "# Fetch a list of all collections\n",
        "all_collections = qdrant_client.get_collections()\n",
        "\n",
        "# Check if the collection name exists in the list of collections\n",
        "if collection_name in all_collections:\n",
        "    # If it exists, delete the current collection\n",
        "    qdrant_client.delete_collection(collection_name=collection_name)\n",
        "    print(f\"Deleted existing collection '{collection_name}'.\")\n",
        "\n",
        "# Define vectors configuration for the new collection\n",
        "vectors_config = models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "\n",
        "# Create the new collection\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=vectors_config\n",
        ")\n",
        "print(f\"Created new collection '{collection_name}'.\")\n",
        "\n",
        "# Now, upsert your embeddings\n",
        "qdrant_client.upsert(points=embeddings_data, collection_name=collection_name)\n",
        "\n",
        "print(f\"Uploaded {len(embeddings_data)} embeddings to Qdrant in the '{collection_name}' collection.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FyaAxr4BaJr",
        "outputId": "ad59bb16-8e3f-4156-c9ca-b98c68463edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new collection 'blutmond_embeddings'.\n",
            "Uploaded 29 embeddings to Qdrant in the 'blutmond_embeddings' collection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_rules(document_id, creds):\n",
        "    \"\"\"\n",
        "    Accesses a Google Docs document with a specific ID and retrieves its content.\n",
        "\n",
        "    Parameters:\n",
        "    - document_id (str): The ID of the Google Docs document.\n",
        "    - creds (Credentials): The credentials used to access Google Drive API.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the Google Docs document.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    # Build the Docs services\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "    # Retrieve the document using its ID\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "\n",
        "    # Extract content from the document\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "\n",
        "    # Combine extracted content into a single string\n",
        "    content_text = \"\".join(content)\n",
        "\n",
        "    return content_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwwLCXz3-Bj7",
        "outputId": "ec9dd01d-0e8e-484b-b3c7-6e6877a56d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spielregeln in Kürze\n",
            "Ein aventurischer Held verfügt über einen bestimmten Satz von Spielwerten: Eigenschaften stellen die geistigen und körperlichen Grundlagen des Helden dar, die durch Übung wenig verändert werden können, die aber bei fast allen Spielentscheidungen eine Rolle spielen. Talente sind erlernte Fertigkeiten des Helden in verschiedenen Bereichen wie Kampf, Naturkunde oder Handwerk; die Erfolgswahrscheinlichkeit eines Talenteinsatzes hängt von den Eigenschaften und dem erlernten Talentwert ab. Schließlich gibt es noch abgeleitete Werte wie den Attacke-Wert, die Lebensenergie oder die Magieresistenz, die sich ebenfalls aus den Eigenschaften bestimmen. Zauberkundige Helden sind in der Lage, Zaubersprüche einzusetzen, was wie der Gebrauch von Talenten funktioniert und ein bestimmtes Maß an Astralenergie kostet, während Geweihte die Kraft ihrer Götter meist in Form von Liturgien umsetzen, die die ihnen von den Göttern verliehene Karmaenergie verbrauchen. Alles, was Sie als Spieler benötigen, sind ein Charakterbogen für die Werte Ihres Helden, wie Sie ihn in diesem Band als Kopiervorlage oder zum Download auf unserer Homepage  ww.ulisses-spiele.de finden, einige weitere Notizzettel, Stifte sowie einen zwanzig seitigen und einen sechsseitigen Würfel. Würfel, Rundungen, Charakterbogen Das Schwarze Auge verwendet zwanzigseitige Würfel (als W20 abgekürzt) und ‘normale’ sechsseitige Würfel (W6). Wenn sich irgendwo die Bezeichnung W (ohne Zahlenangabe) findet, ist damit ebenfalls der W6 gemeint. Wenn wir von einem W3 reden, meinen wir einen Wurf mit dem W6, wobei 1 und 2 als 1 gezählt werden, 3 und 4 als 2 und 5 und 6 als 3. Eine Angabe wie 3W6 bedeutet, dass Sie drei sechsseitige Würfel rollen und die Würfelergebnisse aufaddieren können; eine Angabe 2W20+10 bedeutet, dass Sie zwei zwanzigseitige Würfel rollen, die Ergebnisse zusammenzählen und zusätzlich 10 Punkte addieren. Wenn nicht ausdrücklich anders erwähnt, wird bei allen Rechnungen echt gerundet, d.h. Werte bis n,49 werden ab-, Werte ab n,50 werden aufgerundet. Speziell bei Halbierungen heißt dies, dass aufgerundet wird. Wie bereits oben beschrieben, steht auf einem Charakterbogen (auch Heldendokument genannt) alles, was Sie an Werten für das Spiel brauchen. Eigenschaften und Eigenschaftsproben Es gibt acht (gute) Eigenschaften, die einen Helden charakterisieren: Mut (abgekürzt MU), Klugheit (KL), Intuition (IN), Charisma (CH), Fingerfertigkeit (FF), Gewandtheit (GE), Konstitution (KO) und Körperkraft (KK). Die ersten vier Eigenschaften (MU, KL, IN, CH) werden auch Geistige Eigenschaften genannt, die letzten vier (FF, GE, KO, KK) Körperliche Eigenschaften. Je höher ein Wert eines Helden, desto höher die Chance, dass er eine entsprechende Probe besteht. Eigenschaftswerte bewegen sich für menschenähnliche Wesen in einem Rahmen üblicherweise zwischen 1 und 21, wobei Werte zwischen 8 und 14 den Schwerpunkt bilden. Bei einer Eigenschaftsprobe würfelt der Spieler mit dem 20-seitigen Würfel (1W20); wenn das Resultat nicht höher ist als der Wert der auf die Probe gestellten Eigenschaft, ist die Probe gelungen. Der Spielleiter kann je nach Situation Proben erleichtern oder verschärfen. Eine Erschwerung um +3 bedeutet, dass der Spieler zu seinem Würfelresultat 3 addieren muss und trotzdem unter seinem Eigenschaftswert bleiben, eine Erleichterung von –5 heißt, dass er von seinem Würfelwurf 5 Punkte abziehen darf. Fällt bei der Probe eine 20, gilt dies als Patzer und bedeutet ein besonders schlechtes Resultat, eine gewürfelte 1 wiederum steht für ein besonders gutes Ergebnis. Talentwerte und Talentproben Fertigkeiten wie Klettern, Reiten, Heilen u.ä. bezeichnen wir als Talente; das Maß der Erfahrung, die ein Held in einem Talent besitzt, ist sein Talentwert (kurz: TaW). Üblicherweise bewegt sich der Talentwert in einem Rahmen von 0 bis 21 (mit entsprechenden Ausnahmen von –4 bis +30). In einer Talentprobe fließen die Eigenschaftswerte eines Helden und sein Talentwert zusammen. Um die Probe zu bestehen, legt der Held nacheinander drei Eigenschaftsproben auf die zum Talent gehörigen Eigenschaften ab. Falls der Held über einen positiven Talentwert verfügt, kann er versuchen, mit diesen Talentpunkten (TaP) misslungene Proben auszugleichen. Der Held kann nicht mehr Punkte aus dem TaW-Vorrat nehmen, als darin enthalten sind. Wenn er also schon bei der ersten Eigenschaftsprobe alle Punkte des Talentwertes verbraucht hat, dann stehen ihm für die zweite und dritte Eigenschaftsprobe keine Ausgleichspunkte mehr zur Verfügung. Wenn er aber zu einem späteren Zeitpunkt eine neue Probe auf das gleiche Talent ablegen muss, steht ihm wieder der vollständige Vorrat zur Verfügung. Wie viele Talentpunkte ein Held bei der Probe nicht verbraucht hat (die sogenannten TaP*), bestimmt in vielen Fällen die Qualität der Talentprobe. Auch Talentproben können mit Zuschlägen (Erschwernissen) oder Abzügen (Erleichterungen) versehen werden. In solchen Fällen wird der Talentwert des Helden vor dem Auswürfeln der Probe mit dem Zuschlag oder dem Abzug verrechnet. Dazu wird eine Erschwernis vom Talentwert subtrahiert, eine Erleichterung zum TaW addiert. Wenn ein Held in einem Talent einen negativen Talentwert aufweist oder sein TaW durch situationsabhängige Modifikatoren unter null fällt, so muss er diesen Betrag unter null als Erschwernis zu jedem der drei Würfe der Probe addieren und darf dennoch den jeweiligen Eigenschaftswert nicht übertreffen, wenn die Probe gelingen soll. Ein negativer TaW führt also stets zu drei erschwerten Eigenschaftsproben.  \n",
            "\n",
            "Kampfsystem\n",
            "Die Grundelemente des DSA-Kampfsystems sind der Attacke- und der Paradewert (kurz: AT und PA), die sich aus den Grundwerten (den abgeleiteten Eigenschaften AT- und PA-Basiswert) und Talentwerten (beispielsweise in Säbel oder Schwerter) zusammensetzen und auf die ganz wie bei einer Eigenschaftsprobe – Proben mit einem einzigen W20 abgelegt werden: Der Angreifer würfelt eine Probe auf seinen AT-Wert, der Verteidiger auf seinen PA-Wert, oder kürzer: Der Angreifer würfelt eine Attacke, der Verteidiger eine Parade. Gelingt die Attacke und misslingt die Parade des Gegners, so ist dem Angreifer ein Treffer gelungen, der nun eine bestimmte Anzahl von Trefferpunkten (TP) anrichtet, die von Waffe zu Waffe unterschiedlich sind. Von diesen Trefferpunkten darf der Verteidiger nun noch den Wert seines Rüstungsschutzes (RS) subtrahieren und muss den verbleibenden Rest als Schadenspunkte (SP) von seiner Lebensenergie (LE) abziehen. Ein Held, dessen Lebensenergie auf 0 oder darunter sinkt, ist so gut wie tot; ein Held, dessen LE auf 5 Punkte oder weniger gefallen ist, ist kampfunfähig (und auch unfähig zu zaubern oder die meisten Talente anzuwenden) und kann sich nur mit Mühe bei Bewusstsein halten. Ein besonders heftiger Treffer kann Wunden anrichten, die die Werte des Opfers verschlechtern. Wann ein Kämpfer in einem Gefecht an der Reihe ist, entscheidet sein Initiative-Wert (INI), der zu Beginn des Kampfes aus dem INI-Basiswert und einigen Modifikatoren bestimmt wird. Ein Kampf gliedert sich in Kampfrunden, in denen ein Beteiligter üblicherweise zwei Aktionen (nämlich eine AT und eine PA) zur Verfügung hat. Der Kämpfer mit der höheren INI beginnt den Kampf mit einer AT, auf die der Verteidiger mit einer PA reagiert; dann handelt der Kämpfer mit der nächsthöheren INI, dann der mit der dritthöchsten usf., bis alle Beteiligten eine Attacke ausführen durften. Dann beginnt die nächste Kampfrunde, und der Kampf geht weiter, bis einer der Kämpfer aufgibt, tot oder kampfunfähig ist – oder bis sein Gegner freiwillig von ihm ablässt. Um seine Chancen im Kampf zu verbessern, stehen dem Helden bestimmte Manöver zur Verfügung, die er durch das Erlernen von Sonderfertigkeiten erwerben kann. Im Fernkampf würfelt der Schütze auf seinen Fernkampf-Wert, der sich aus dem Fernkampf-Basiswert und dem Talentwert in der entsprechenden Fernwaffen-Gattung (Bogen, Wurfspeer etc.) bestimmt. Die Wahrscheinlichkeit, ein Ziel zu treffen, hängt vor allem von der Entfernung ab. Ein Treffer mit einer Fernwaffe erzeugt ebenfalls Trefferpunkte (s.o). Lebenspunkte, Ausdauer, Astralpunkte und Karmapunkte Neben den Eigenschaften des Helden, seinen Talenten und seinen Kampfwerten verfügt jeder Held noch über mindestens zwei weitere Werte, die seine körperliche Verfassung widerspiegeln: die Lebensenergie (LE; unterteilt in Lebenspunkte, LeP), die repräsentiert, ob ein Held kleinere Blessuren oder größere Wunden erlitten hat, und die anzeigt, ob er dem Leben oder dem Tode näher steht, und die Ausdauer (AU; untergliedert in Ausdauerpunkte, AuP), die anzeigt, wie sehr ein Held ‘außer Puste’ ist und ob er noch weitere körperliche Anstrengungen unternehmen kann. Diese beiden Werte berechnen sich zu Spielbeginn aus den Eigenschaften. Zauberkundige Helden verfügen darüber hinaus noch über die Astralenergie (AE; unterteilt in Astralpunkte, AsP), Helden, die Geweihte einer der aventurischen Gottheiten sind, verfügen neben der Lebensenergie und der Ausdauer noch über die sogenannte Karmaenergie (KE; unterteilt in Karmapunkte, KaP), ein göttliches, schwer fassbares Fluidum, mit dem die Geweihten Wunder wirken und so den göttlichen Willen auf Dere repräsentieren können. Wenn ein Held durch Kämpfe Lebensenergie verloren hat, durch Anstrengung Ausdauerpunkte oder durch Zauber Astralenergie, sind diese Punkte nicht auf Dauer verloren, sondern regenerieren sich, während der Held ausruht. Karmaenergie kann durch Meditation zurückgewonnen Werden.\n",
            "\n",
            "Zauberei und Götterwirken\n",
            "Zauberei begegnet den Aventuriern in Form von Zaubersprüchen, magischen Ritualen und Artefakten und in der Person von Magiern, Elfen und anderen magisch begabten Personen wie Hexen oder Druiden und magischen Lebewesen wie Drachen und Einhörnern. Damit ein Lebewesen in Aventurien Zauberei anwenden kann, muss es von Geburt an dazu begabt sein, Astralenergie in sich aufzunehmen. Mit einem Zauberspruch wird diese Energie in einen magischen Effekt nach dem Willen des Zauberers umgewandelt; jeder Zauberspruch benötigt also gewisse Menge Astralenergie. Um einen Zauberspruch zu wirken, benötigt ein Magieanwender Konzentration: Dazu legt er eine Zauber-Probe ab, die wie eine Talentprobe gehandhabt wird. Für diese Probe steht dem Zauberer je nach Spruch eine bestimmte Menge an Zauberfertigkeitspunkten (ZfP) zur Verfügung. Wird der Zauber gegen eine Person gewirkt, so gilt deren Magieresistenz (MR) als Erschwernis. Magische Rituale sind meist mit höherem Zeitaufwand durchzuführende Zaubersprüche, die dafür aber auch meist eine längerfristige Wirkung zeigen, zum Beispiel die Beschwörung von Dämonen oder die Verzauberung von Gegenständen zu magischen Artefakten. Letztere stellen gewissermaßen eine Form von gespeicherter Zauberei dar und sind nicht selten auch für ‘weltliche’, das heißt: nicht mit arkanen Kräften gesegnete Personen verwendbar. Aventurische Geweihte erhalten von ihren Göttern Karmaenergie verliehen, die sie in Form von Mirakeln oder Liturgien einsetzen können. Mirakel verbessern bestimmte Talentwerte des Helden, während Liturgien ähnlich wie Zauber eine übernatürliche Wirkung hervorrufen. Für beide Effekte ist eine sogenannte Mirakelprobe nötig, die wie eine Talentprobe gehandhabt wird, wobei der Liturgiekenntniswert (LkW) den Talentwert ersetzt.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####SEARCH FILES####\n",
        "\n",
        "# Example usage:\n",
        "Frage = \"Wie funktioniert DSA\" #@param {type: \"string\" }\n",
        "Anzahl_zu_durchsuchender_Dokumente = \"5\" #@param {type: \"string\" }\n",
        "user_input = Frage\n",
        "\n",
        "oauth2_service = build('oauth2', 'v2', credentials=creds)\n",
        "\n",
        "# Get user info\n",
        "user_info = oauth2_service.userinfo().get().execute()\n",
        "user_email = user_info['email']\n",
        "\n",
        "def get_base_rules(document_id, creds):\n",
        "    \"\"\"\n",
        "    Accesses a Google Docs document with a specific ID and retrieves its content.\n",
        "\n",
        "    Parameters:\n",
        "    - document_id (str): The ID of the Google Docs document.\n",
        "    - creds (Credentials): The credentials used to access Google Drive API.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the Google Docs document.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    # Build the Docs services\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "    # Retrieve the document using its ID\n",
        "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
        "\n",
        "    # Extract content from the document\n",
        "    content = []\n",
        "    for element in doc.get('body', {}).get('content', []):\n",
        "        content.extend([part.get('textRun', {}).get('content', '')\n",
        "                        for part in element.get('paragraph', {}).get('elements', [])])\n",
        "\n",
        "    # Combine extracted content into a single string\n",
        "    content_text = \"\".join(content)\n",
        "\n",
        "    return content_text\n",
        "\n",
        "# Example usage of the get_document_content function\n",
        "document_id = '1gA3Xicio9d6vZklPQnFE97M_Ajil4vxqJClEQDR4j-8'\n",
        "base_rules = get_base_rules(document_id, creds)\n",
        "\n",
        "# Generate a timestamp\n",
        "def generate_timestamp(timezone=\"CET\"):\n",
        "    \"\"\"\n",
        "    Generate a timestamp in the specified timezone.\n",
        "\n",
        "    Parameters:\n",
        "    - timezone: str, the timezone in which the timestamp is generated.\n",
        "\n",
        "    Returns:\n",
        "    - str, the timestamp.\n",
        "    \"\"\"\n",
        "    tz = pytz.timezone(timezone)\n",
        "    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def query_text(user_input, model=\"text-embedding-ada-002\", top_k=Anzahl_zu_durchsuchender_Dokumente):\n",
        "    \"\"\"\n",
        "    Query the vector database based on user input text.\n",
        "\n",
        "    Parameters:\n",
        "    - user_input: str, text input from the user to query against the vector database.\n",
        "    - model: str, the name of the OpenAI model used for embedding creation.\n",
        "    - top_k: int, the number of closest matches to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - list of dict, containing the top_k most similar documents to the user input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for user input\n",
        "    user_embedding = openai.Embedding.create(\n",
        "        input=[user_input], engine=model)['data'][0]['embedding']\n",
        "\n",
        "    # Perform query to find nearest neighbors in the vector database\n",
        "    results = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        limit=top_k,  # Retrieve top_k most similar points\n",
        "        query_vector=user_embedding,\n",
        "        with_payload=True  # Get the payload data along with the results\n",
        "    )\n",
        "\n",
        "    # Extract and return useful information from the results\n",
        "    similar_docs = [\n",
        "        {\n",
        "            \"id\": result.id,\n",
        "            \"payload\": result.payload,\n",
        "            \"score\": result.score,\n",
        "        }\n",
        "        for result in results\n",
        "    ]\n",
        "\n",
        "    return similar_docs\n",
        "\n",
        "\n",
        "ranked_results = query_text(user_input)\n",
        "\n",
        "# Print or further process the results as needed\n",
        "print(json.dumps(ranked_results, indent=4))  # Pretty print the results with indentation\n",
        "\n",
        "# Extract text content from the payloads into a list\n",
        "text_contents = []\n",
        "titles = []\n",
        "scores = []\n",
        "for doc in ranked_results:\n",
        "    payload = doc.get('payload', {})\n",
        "    text_content = payload.get('content_text', '')  # Replace with actual key if different\n",
        "    title = payload.get('content_name', '')  # Replace with actual key if different\n",
        "    score = doc.get('score', 0)\n",
        "\n",
        "    text_contents.append(text_content)\n",
        "    titles.append(title)\n",
        "    scores.append(score)\n",
        "\n",
        "# Now text_contents is a list of the text content from the ranked_results\n",
        "print(\"Extracted text contents:\")\n",
        "print(text_contents)\n",
        "\n",
        "# Generate an answer based on a file\n",
        "def generate_answer(user_input, text_contents):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\n",
        "            f\"role\": \"system\", \"content\": # setup und high level context\n",
        "            f\"Du bist ein Regelexperte und Spielleiter (eng.: Dungeon Master) einer Pen-&-Paper-Rollenspiel Kampagne im Das Schwarze Auge (DSA) Regelsystem.\"\n",
        "            f\"Dies sind die Basis Regeln: {base_rules}\"\n",
        "            f\"Bitte beantworte die Fragen mit entsprechenden detaillierten Erklärungen wie die Regeln funktionieren und an zu wenden sind.\"\n",
        "            f\"Bleibe jedoch faktisch genau bei den Regeln und erfinde nichts dazu.\"\n",
        "            f\"Antworte kurz und bündig in drei Sätzen.\"\n",
        "            },\n",
        "            {\n",
        "              \"role\": \"user\", \"content\": # prompt\n",
        "             f\"Beantworte die Frage: '{user_input}', basierent auf deinem Wissen und diesem Dokument: \\n{text_contents} und der Basis Regeln\"\n",
        "             }\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message[\"content\"].strip()\n",
        "    return answer\n",
        "\n",
        "answer = generate_answer(user_input, text_contents)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "display(Markdown(\"### Frage: \"))\n",
        "display(Markdown(f\"_{user_input}_\\n\"))\n",
        "display(Markdown(\"### Antwort: \"))\n",
        "display(Markdown(f\"_{' '.join(answer.split())}_\"))\n",
        "\n",
        "# Additional Payload display:\n",
        "ref_texts = []\n",
        "for doc, title in zip(ranked_results, titles):\n",
        "    payload = doc.get('payload', {})\n",
        "    name = payload.get(\"name\", \"\")\n",
        "    abbr = payload.get(\"abbreviation\", \"\")\n",
        "    page = payload.get(\"page_number\", \"\")\n",
        "    cont = payload.get(\"continued\", False)\n",
        "    url = payload.get(\"source_book_url\", \"\")\n",
        "\n",
        "    # Construct reference string with Markdown formatting for hyperlink\n",
        "    cont_text = \"ff\" if cont else \"\"\n",
        "    ref_text = f\"[{name}]({url}) ({abbr} S. {page}{cont_text})\"\n",
        "    ref_texts.append(ref_text)\n",
        "\n",
        "# Concatenate reference texts and display\n",
        "ref_str = \", \".join(ref_texts)\n",
        "display(Markdown(\"### Regel Referenzen zum Nachschlagen: \"))\n",
        "display(Markdown(ref_str))\n",
        "\n",
        "# Load the existing QA pairs from the JSON file if it exists\n",
        "try:\n",
        "    with open(f'{app_dir}/{company_name}_qa_history.json', 'r') as file:\n",
        "        qa_history = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    # If file doesn't exist, initialize an empty list\n",
        "    qa_history = []\n",
        "\n",
        "# Append the new QA pair and save to file\n",
        "qa_pair = {\"user\": user_email,\n",
        "           \"timestamp\": generate_timestamp(),\n",
        "           \"question\": Frage,\n",
        "           \"answer\": answer,\n",
        "           \"returned_amount\": Anzahl_zu_durchsuchender_Dokumente,\n",
        "           \"references\": [{\"Titel\": t, \"Score\": s} for t, s in zip(titles, scores)]\n",
        "           }\n",
        "qa_history.append(qa_pair)\n",
        "\n",
        "# Save the updated QA pairs back to the JSON file\n",
        "with open(f'{app_dir}/{company_name}_qa_history.json', 'w') as file:\n",
        "    json.dump(qa_history, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Confirming the action\n",
        "display(Markdown(\"<sub>Logs gespeichert.</sub>\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "iinyoB8K1ws3",
        "outputId": "68124d33-57ac-492c-910f-bcb7a9057a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Frage: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Wie funktioniert DSA_\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Antwort: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_\"Das Schwarze Auge\" ist ein pen and paper Rollenspiel, in dem Spieler Charaktere in einer fiktiven Welt spielen. Jeder Charakter hat spezifische Eigenschaften, Talente und abgeleitete Werte, die das Ergebnis von Aktionen bestimmen, welche durch Würfelwürfe repräsentiert werden. Kämpfe werden durch Attacke- und Paradewerte geregelt, Magie durch Astralenergie und göttliche Aktionen durch Karmaenergie._"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Regel Referenzen zum Nachschlagen: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[Ausfall](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 277), [Defensiver Kampfstil](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Doppelangriff](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278), [Standfest](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 277), [Beidhändiger Kampf I](https://drive.google.com/file/d/194Mi9H0bAZWiGVDDylJBtCiOySLZHkyA/view?usp=sharing) (WdH S. 278)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<sub>Logs gespeichert.</sub>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Em5HQ6kATbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}